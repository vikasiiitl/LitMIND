PDF File Name,Merged Summary
0,"Kahng et al. discuss ORION3.0, an advanced open-source NoC router estimation tool that significantly improves upon ORION2.0 by utilizing actual physical implementation data. ORION3.0 achieves average estimation errors of no more than 9.3%, offering more accurate predictions across various parameters and router RTL generators. This tool employs both parametric and non-parametric modeling techniques, providing reliable estimations for NoC router designs and enhancing accuracy in microarchitecture and implementation aspects [1]."
1,"Cheng et al. introduce ""DAGSizer,"" employing a directed graph convolutional network to optimize discrete gate sizing in VLSI graphs. This method enhances computational efficiency and accuracy, leveraging deep learning for challenges in VLSI circuit optimization, thus promising significant advancements in electronic design automation [2]."
2,"Li et al. proposed the NoCeption framework, utilizing Graph Neural Networks (GNNs) to predict Power, Performance, and Area (PPA) of Network-on-Chips (NoCs). This framework models NoCs as attributed graphs, achieving over 97% accuracy in power and area estimation and enhancing network-level and system-level performance by 6.52% and 4.73%, respectively. NoCeption demonstrates significant improvements in handling complex NoC topologies compared to traditional methods [3]."
3,"Sengupta et al. emphasize the need for the specific text of an abstract to generate an accurate summary, highlighting the inability to proceed without it. They suggest providing the abstract text for effective assistance in crafting a summary [4]."
4,"Dai et al. propose machine learning models to improve the accuracy of quality of results (QoR) estimation in high-level synthesis (HLS) for FPGA designs. These models address discrepancies between estimated and actual QoR, significantly reducing estimation errors and enhancing design space exploration. The study utilizes a comprehensive dataset, which has been made public to further research in this area [5]."
5,"Felix Last and Ulf Schlichtmann demonstrated that transfer learning significantly reduces the labeled data required for training PPA estimation models in embedded memories. By exploiting similarities across memory compiler versions and technology nodes, this method cuts the time to provision new models by 50% to 90%, reducing the sample size from 13,000 to 1,300 and shortening provisioning times from over 40 days to less than a week, thus accelerating critical design cycles [6]."
6,"Esmaeilzadeh et al. introduced an open-source, ML-based optimization framework aimed at enhancing machine learning accelerator performance. The framework, detailed in terms of architecture, implementation, and potential impacts, seeks to optimize machine learning computations and encourage broader adoption and innovation within the field. The text covers the framework's comprehensive features and its role in improving efficiency in machine learning tasks [7]."
7,"Xu et al. developed AutoDNNchip, an automated tool for designing DNN chips for FPGAs and ASICs. It includes a Chip Predictor that accurately forecasts performance metrics and a Chip Builder that optimizes design and generates synthesizable RTL code. This tool significantly reduces design time and can achieve up to 3.86 times better performance than expert-designed accelerators. AutoDNNchip is open-source and available on GitHub, facilitating rapid and effective DNN chip design without human intervention [8]."
8,"Li et al. introduce McPAT, an integrated framework for modeling power, area, and timing in multicore and manycore processors. It supports in-order and out-of-order cores, networks-on-chip, and shared caches, covering microarchitectural, circuit, and technology levels. McPAT enables design space exploration and performance trade-off assessments using metrics like EDA2P and EDAP. Studies highlight that clustering cores enhances performance but increases area overhead, with 4-core clusters being optimal at the 22nm technology node for cost-efficiency [9]."
9,"Lee et al. propose PowerTrain, a methodology that enhances McPAT power models by calibrating them with real hardware measurements, significantly improving prediction accuracy. This approach is particularly effective in scenarios with fluctuating workloads, demonstrated using the Cortex-A15 within the Samsung Exynos 5422 SoC, achieving mean percentage errors of 2.04% and absolute errors of 4.37%. The calibration aligns McPAT's estimates closely with actual power consumption, leading to more precise predictions [10]."
10,"Orion, a novel power-performance simulator, addresses the critical need for evaluating trade-offs in interconnection network design, crucial for server blades and SoCs. It integrates parameterized power models and a synthesized simulator, enabling architects to optimize network parameters effectively under various conditions. Orion facilitates early assessment and optimal design decisions, enhancing microarchitectural efficiency [11]."
11,"Manasi et al. formulated a new analytical CNN energy model for deep learning accelerators, benchmarked against silicon data. NeuPart, the introduced model, optimizes energy consumption by partitioning CNN computations between local processing and cloud offloading. This approach considers energy usage in both the mobile client and cloud, including data transmission. NeuPart achieves up to 73.4% energy savings compared to fully cloud-based or in-situ computations for various CNN topologies [12]."
12,"Lin et al. developed HL-Pow, a machine learning-based power modeling framework for FPGA HLS, which automates feature extraction and utilizes a modeling flow to build accurate power models. HL-Pow expedites power evaluation by leveraging HLS over RTL implementation and achieves a power modeling accuracy close to onboard measurements with only a 4.67% discrepancy. It also incorporates a novel DSE algorithm that optimizes the trade-off between latency and power consumption, enhancing FPGA design efficiency and achieving near-optimal solutions with fewer HLS simulations [13]."
13,"Shao et al. discuss Aladdin, a pre-RTL accelerator simulator that estimates performance, power, and area with high accuracy (within 0.9%, 4.9%, and 6.6% of RTL implementations). Aladdin facilitates extensive design space exploration by integrating with core and memory hierarchy simulators, allowing for modeling of accelerators' impact on SoC power and performance. This tool offers a faster, less labor-intensive alternative to traditional RTL-based synthesis, enhancing performance and energy efficiency in system-on-chip simulations [14]."
