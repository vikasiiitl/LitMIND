PDF File Name,Summary_T5,Summary_Gemini,Summary_Gpt4,Summary_Gpt3,Summary_BART
0,flaky test cases can lead to unnecessary attempts to debug production code . flakify relies exclusively on the source code of test cases .,"Flakify is a novel black-box predictor for flaky test cases that relies solely on test case source code. Using a fine-tuned language model, Flakify achieves high accuracy in predicting flaky test cases, outperforming the state-of-the-art on two public datasets. Flakify reduces the need for manual debugging and rerunning, saving significant time and resources. Unlike existing predictors, Flakify does not require access to production code or pre-defined features, making it applicable to a wide range of projects.","Flakify, a black-box, language model-based predictor, utilizes CodeBERT to identify flaky test cases solely from their source code, eliminating the need for production code access or feature pre-definition. It outperforms the state-of-the-art FlakeFlagger, achieving higher F1-scores and demonstrating better generalizability across different datasets and validation methods.","""Flakify is a black-box, language model-based predictor for flaky test cases, eliminating the need for pre-defined features or access to production code. It outperforms the state-of-the-art FlakeFlagger approach, achieving higher F1-scores and reducing debugging costs by up to 64%. Flakify demonstrates better generalizability in predicting test cases across new projects.""","Flakify: A Black-Box, Language Model-Based Predictor for Flaky TestsSoftware testing assures that code changes do not adversely affect existing functionality. However, a test case can be flaky, i.e., passing"
1,a systematic literature review (SLR) on large language models for software engineering (LLMs) focuses on how LLMs can be exploited to optimize processes and outcomes . a systematic literature review (SLR),"A comprehensive systematic literature review explores the application of Large Language Models (LLMs) to Software Engineering (SE) tasks. The review categorizes LLMs, examines data and method usage, analyzes optimization and evaluation strategies, and identifies successful SE applications. Findings reveal the potential of LLMs to optimize processes and outcomes in SE, while highlighting areas for further research and development.","This systematic literature review on Large Language Models (LLMs) in Software Engineering (SE) analyzes 395 papers to understand LLM applications, effects, and limitations in SE. It categorizes LLMs, examines data methodologies, evaluates performance strategies, and identifies successful SE tasks, providing a foundation for future research and practical implementations.","This systematic literature review explores the impact of Large Language Models (LLMs) on Software Engineering (SE) tasks. Analyzing 395 research papers, it categorizes LLMs used in SE, examines data collection methods, optimization strategies, and successful applications. The study identifies gaps, trends, and future research directions, offering valuable insights for optimizing SE processes.","Large Language Models for Software Engineering: A Systematic Literature Review Large Language Models (LLMs) have significantly impacted numerous domains, including Software Engineering (SE). Many recent publications have explored LLMs applied to various SE tasks. Nevertheless, a"
2,NoRBERT is a language model that has proven useful for transfer learning . it can be applied to unseen projects with convincing results .,"NoRBERT, a transfer learning model based on BERT, enhances requirements classification accuracy. It effectively handles variations in project-specific language, outperforming existing approaches. NoRBERT achieves high performance (F1-score up to 94%) on both seen and unseen projects, demonstrating its generalization capabilities. Additionally, it effectively classifies non-functional requirement subclasses and functional concerns, further improving requirements classification.","NoRBERT, a fine-tuned BERT model, significantly enhances the classification of functional and non-functional requirements, achieving F1-scores up to 94%. It excels in generalization across unseen projects, outperforming existing methods with a 15 percentage point improvement in F1-scores, demonstrating robust performance in diverse requirements classification tasks.","NoRBERT, a fine-tuned BERT model, enhances requirements classification by addressing poor generalization in varying project contexts. Achieving F1-scores up to 94%, it outperforms recent approaches in classifying functional and non-functional requirements, with a notable 15% improvement on unseen projects. NoRBERT also excels in classifying functional requirements based on concerns, showcasing its robustness across diverse scenarios.",NoRBERT: Transfer Learning for Requirements ClassificationClassifying requirements is crucial for automatically handling natural language requirements. The performance of existing automatic classification approaches diminishes when applied to unseen projects because requirements usually vary in wording and style. The main
3,XAI is a new method to make AI decisions more understandable . it uses a Python code execution function to make decisions .,"Explainable Artificial Intelligence (XAI) enhances the interpretability of AI outputs in medicine, where transparency is crucial. This study introduces the Code Base Prompt (CBP) for ChatGPT Code interpreter to make medical decision-making transparent through Python code execution. The Medical Algorithm Presentation Criteria (MAPC) evaluates the explainability of medical algorithms applied to medical texts. Experiments demonstrate that CBP outperforms Text Base Prompt (TBP) in meeting MAPC criteria, providing a novel approach to XAI in ChatGPT for medical tasks.","The study introduces a novel Explainable Artificial Intelligence (XAI) approach for medical applications using ChatGPT, termed Code Base Prompt (CBP), which enhances transparency by rewriting medical decision-making algorithms in Python code. This method, evaluated against Text Base Prompt (TBP) through heart failure classification tasks, demonstrated superior explainability and alignment with human understanding processes, significantly outperforming TBP in executing and explaining decisions.","The study explores the importance of Explainable Artificial Intelligence (XAI) in medicine, focusing on making ChatGPT's decision-making process transparent. Introducing the Code Base Prompt (CBP) and Medical Algorithm Presentation Criteria (MAPC), the research demonstrates how CBP enhances interpretability by executing Python code, aligning with all five MAPC factors, unlike the Text Base Prompt (TBP).","XAI for Medicine by ChatGPT Code interpreterIn recent years, with the prevalence of Artificial Intelligence (AI), the interpretability of AI outputs has become a significant issue. Especially the interpretationability of large language models (LLMs),"
4,"only a small subset of production systems is studied, limiting the impact of this research . large language models (LLMs) such as GPT-4 could help replicate empirical software engineering research .","Large language models (LLMs) like GPT-4 have the potential to enhance empirical software engineering research by replicating studies on production systems. This could democratize research, making it accessible to practitioners who lack the expertise to conduct replications themselves. However, these models require a deep understanding of research methodologies and software engineering data nuances to be effective.","The text explores how GPT-4 could revolutionize empirical software engineering research by aiding in the replication of studies on production systems, which is currently limited and challenging. This could enhance understanding and democratize knowledge for practitioners, despite the complexities involved in research methodologies and data nuances.","Empirical software engineering research has enhanced understanding of the software engineering process, yet limited by studying only a small subset of production systems. Replicating research on their own data could benefit practitioners, but poses challenges. Large language models like GPT-4 offer promise in democratizing this research by aiding in replications.","Can GPT-4 Replicate Empirical Software Engineering Research?Empirical software engineering research on production systems has brought forth a better understanding of the software engineering process for practitioners and researchers alike. However, only a small subset"
5,a new evaluation set is released to measure functional correctness for synthesizing programs . repeated sampling from the model is a surprisingly effective strategy for producing working solutions .,"Codex, a GPT model trained on GitHub code, exhibits strong Python code-writing capabilities. On the HumanEval dataset, it solves 28.8% of problems, outperforming other GPT models. Repeated sampling enhances performance significantly, allowing it to solve 70.2% of problems. However, limitations include handling long operation chains and variable binding. The deployment of powerful code generation technologies raises ethical concerns regarding safety, security, and economic implications.","The research introduces Codex, a GPT model fine-tuned on GitHub code, significantly outperforming previous models in Python code synthesis. Codex, powering GitHub Copilot, solves 28.8% of HumanEval set problems, and with repeated sampling, achieves a 70.2% solution rate. The study also discusses the broader impacts of such technologies.","Codex, a GPT language model trained on GitHub code, outperforms GPT-3 and GPT-J in Python code-writing tasks, solving 28.8% of problems. Repeated sampling from Codex yields a 70.2% success rate. Limitations include challenges with complex docstrings and variable binding. The study highlights the implications of advanced code generation technologies on safety, security, and economics.","Evaluating Large Language Models Trained on CodeAbstractWe introduce Codex, a GPT language model fine-tuned on publicly available code from GitHub, and study its Python code-writing capabilities. A distinct production version of Codex powers"
6,SpecSyn is a framework that leverages a state-of-the-art large language model . it can automatically synthesize software specifications from natural language sources .,"SpecSyn automates software specification synthesis from natural language sources using a large language model. It formulates synthesis as a sequence-to-sequence learning problem, outperforming existing tools by 21% in F1 score. SpecSyn can extract specifications from single or multiple sentences, addressing the challenge of scattered and disorganized specifications in external sources.","The SpecSyn framework utilizes advanced large language models to automatically generate software specifications from diverse natural language sources, addressing the challenge of managing complex software configurations. This innovative approach, a first in the field, significantly improves specification accuracy, outperforming existing tools by 21% in F1 score.","""Software specifications are crucial for safe operation, but manual handling becomes challenging with complex systems. Existing specifications are often scattered across sources, hindering clarity. Introducing SpecSyn, a framework utilizing large language models to automatically synthesize specifications from natural language texts. This innovative approach outperforms existing tools by 21% in F1 score.""","Large Language Models Based Automatic Synthesis of Software SpecificationsSoftware configurations play a crucial role in determining the behavior of software systems. In order to ensure safe and error-free operation, it is necessary to identify the correct configuration, along with their"
7,"program merging is standard practice when developers integrate their changes to a common code base . when the merge algorithm fails, this is called a merge conflict . k-shot learning with pre-trained large neural language models (","Pre-trained language models (LMs) are explored for automatic resolution of textual and semantic merge conflicts. While LMs achieve state-of-the-art performance on semantic merge conflicts, they fall short of specialized domain-specific languages for textual conflict resolution. The study demonstrates the feasibility of LM-based conflict resolution but highlights the need for further research for comprehensive solutions.","This paper investigates using pre-trained neural language models like GPT-3 to automatically resolve textual and semantic merge conflicts in software development. It explores k-shot learning's effectiveness within limited prompt sizes and presents mixed results: while improving semantic conflict resolution for Microsoft Edge, it doesn't surpass domain-specific languages for certain synthesis tasks.","This paper explores using pre-trained large neural language models like GPT-3 to automatically resolve textual and semantic merge conflicts in code projects. While these models show state-of-the-art performance in semantic conflict resolution for Microsoft Edge, they do not fully replace domain-specific languages for certain program synthesis patterns.","Using Pre-trained Language Models to Resolve Textual and Semantic Merge Conflicts (Experience Paper)ABSTRACTProgram merging is standard practice when developers integrate their individual changes to a common code base. When the merge algorithm fails,"
8,GPT-4 is a transformer-based model pre-trained to predict the next token in a document . the model passes a simulated bar exam with a score around the top 10% of test takers .,"GPT-4, a multimodal model from OpenAI, leverages both image and text inputs to generate text outputs. Despite its limitations, GPT-4 showcases impressive performance in professional and academic domains, even passing a simulated bar exam within the top 10% of human test takers. The underlying Transformer model undergoes post-training alignment, optimizing factuality and adherence to desired behavior. GPT-4's development involved scalable infrastructure and optimization methods, enabling accurate performance predictions based on smaller models.","The GPT-4 technical report introduces a multimodal, large-scale model capable of processing both image and text inputs to generate text outputs. Achieving human-level performance on several benchmarks, GPT-4 excels in professional and academic areas, notably scoring in the top 10% on a simulated bar exam. The model's development focused on infrastructure and optimization for predictable scaling and enhanced performance in factuality and desired behavior adherence.","GPT-4 is a large-scale multimodal model that excels in professional and academic benchmarks, achieving human-level performance in various scenarios, including a simulated bar exam. Utilizing a Transformer-based architecture, GPT-4's post-training alignment process enhances factuality and behavior adherence. Infrastructure development enables accurate performance prediction with minimal computational resources.","GPT-4 Technical ReportWe report the development of GPT- 4, a large-scale, multimodal model which can accept image and text inputs and produce text outputs. While less capable than humans in many real-world"
9,a new approach to API recommendation is proposed by a team of researchers . the approach uses a BERT-based sentence embedding model . the model can learn precise semantic representation of programming terminologies .,"CLEAR, a novel API recommendation approach, tackles two limitations of existing methods: bag-of-words representation and weak semantic distinction. CLEAR leverages BERT sentence embedding and contrastive learning to capture sequential information and enhance semantic representation. After selecting candidate SO posts based on sentence embedding similarity, CLEAR employs a BERT-based re-ranking model to recommend APIs from the ranked top posts. Experiments on three datasets demonstrate CLEAR's superiority, improving MAP by up to 25%-187% at the method-level and 10%-100% at the class-level.","The paper introduces CLEAR, a novel API recommendation system that utilizes BERT sentence embeddings and contrastive learning to address deficiencies in traditional IR-based and neural-based methods. CLEAR enhances semantic understanding and distinguishes between similar queries more effectively, significantly outperforming existing approaches in method-level and class-level API recommendation accuracy.","The paper introduces CLEAR, a novel API recommendation system that utilizes BERT sentence embedding and contrastive learning to address limitations in existing methods. By embedding queries and Stack Overflow posts with BERT, CLEAR captures sequential semantic information and enhances semantic distinction among lexically similar queries. Experimental results demonstrate CLEAR's significant performance improvement over state-of-the-art approaches.","CLEAR: Contrastive Learning for API RecommendationAutomatic API recommendation has been studied for years. There are two orthogonal lines of approaches for this task, i.e., information-retrieval-based (IR-"
10,large language models (LLMs) have drawn widespread attention and research . they have been extensively deployed and highly sought after . the evaluation and optimization of LLMs in software engineering tasks have become essential .,"Large Language Models (LLMs) have gained attention in software engineering tasks, particularly code generation. However, their evaluation and optimization in this context are crucial, necessitating research on how to effectively utilize LLMs for software engineering tasks.","The research focuses on Large Language Models (LLMs) like ChatGPT, highlighting their exceptional performance in text generation and reasoning. It emphasizes the growing importance of evaluating and optimizing LLMs for software engineering tasks, including code generation, reflecting their increasing integration and significance in the field.","Large Language Models (LLMs) have gained significant traction for their exceptional performance in text generation and reasoning tasks, leading to the popularity of derivative products like ChatGPT. The evaluation and enhancement of LLMs for software engineering tasks, particularly in code generation, have emerged as crucial areas of focus in research and development.","Title: Towards an Understanding of Large Language Models in Software Engineering TasksAbstract: Large Language models (LLMs) have drawn widespread attention and research due to their astounding performance in text generation and reasoning tasks. Derivative products, like"
