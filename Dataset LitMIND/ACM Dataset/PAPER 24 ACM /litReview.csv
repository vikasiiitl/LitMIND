Related_Work
"Numerous studies have addressed the application of NLP techniques in requirements engineering (RE), focusing on tasks like classification and ambiguity detection. However, replication in NLP for RE research is limited due to factors such as context specificity and task heterogeneity. This paper introduces the ID-Card artifact to aid in structuring replication-relevant information, thereby promoting study replication and educational use.Ahmed et al. conducted a systematic review of 70 papers on the automatic transformation of natural language to Unified Modeling Language (UML), identifying key limitations and benefits of current methods. The review emphasizes the challenges of natural language processing and the need for a common dataset and evaluation framework to enhance research effectiveness. It suggests directions for future research to address issues like ambiguity and incompleteness in existing approaches [1]. Shah et al. proposed a rule-based approach, SAFE, for extracting app features from user reviews, which was initially evaluated subjectively. An independent study replicated this evaluation using an objective and repeatable method across eight datasets, revealing lower precision and recall than originally reported, indicating the need for further investigation into SAFE's effectiveness and generalizability. The replication emphasized the importance of transparency and reproducibility in research by sharing their methods and datasets [2]. Zhao et al. discuss the evolution and impact of large language models (LLMs), emphasizing their transformation from statistical to neural models, particularly through the advancement of pre-trained Transformer models like ChatGPT. These models, significantly larger in scale, demonstrate improved performance and unique capabilities such as in-context learning, revolutionizing natural language processing and AI development. The survey also explores key aspects of LLMs including pre-training, tuning, utilization, and capacity evaluation, serving as a comprehensive reference for researchers and developers [3]. González-Barahona & Robles investigate the reproducibility of empirical software engineering studies, highlighting the variability from easy to nearly impossible. They propose a systematic methodology to assess and enhance reproducibility, identifying key elements and challenges such as complex processes and insufficient detail in study descriptions. This approach aims to improve understanding and facilitate the reproduction of studies using development repository data [4]. Anchundia & Fonseca C. investigated reproducibility in empirical software engineering, analyzing over 2,600 studies. They found that despite the availability of tools, reproducibility challenges persist due to low adoption and usability issues. The study revealed that tool selection varies with the experiment type; technology-oriented experiments use automated, specialized tools, whereas human-oriented experiments rely on costly informal mechanisms. The results suggest a need for broader, adaptable replication strategies to enhance reproducibility in the field [5]. Abualhaija et al. discuss the challenges in replicating NLP studies in Requirements Engineering (RE) due to context specificity, task heterogeneity, and varying reporting formats. They propose the ID-Card, a structured artifact designed to facilitate replication by providing essential information for data annotation and tool reconstruction. This initiative aims to enhance awareness and support replication efforts in NLP applications within RE, addressing the noted lack of attention to replication in this field [6]. Kolahdouz-Rahimi et al. conducted a systematic literature review on the use of NLP and ML for RF in software development, analyzing 47 studies from 2012-2022. The review reveals that heuristic NLP approaches are frequently used, while Deep Learning is underutilized. It also notes that classical ML techniques are more common and highlights the challenge of comparing performance due to the absence of standard benchmarks in RF [7]. Magalhães et al. conducted a systematic mapping study on the replication of empirical studies in software engineering, highlighting its importance for validation and generalization of findings. The study reviewed 108 primary studies, offering insights into research methods, challenges, and opportunities to enhance replicability in the field [8]. Silva et al. conducted a systematic mapping study analyzing 133 replications of empirical software engineering studies from 1994 to 2010, revealing that 70% of these replications occurred after 2004, primarily internally, with a focus on software requirements, construction, and quality. Despite an increase in replication efforts, the overall number remains low, highlighting the need for enhanced reporting standards and incentives to encourage more external replications and collaborative research [9]. Mendez et al. discuss the integration and challenges of implementing open science in software engineering, emphasizing its slow adoption compared to other disciplines. They explore the benefits and practical strategies for open science, drawing on their experiences as conference chairs and researchers. The chapter aims to promote open science as a norm in the field, addressing challenges like preprint sharing and licensing [10]. Dąbrowski et al. investigate automated methods for analyzing app reviews in requirement engineering, assessing opinion mining and feature-specific search techniques. Their findings reveal that these approaches are less effective than previously reported, highlighting significant discrepancies in effectiveness and questioning their practical utility. The study emphasizes the challenges in replicating studies due to varying methods and datasets, suggesting a reevaluation of these automated techniques [11]. Gonzalez-Barahona & Robles revisited their 2012 method for assessing reproducibility in empirical software engineering studies, focusing on data from development repositories. Their analysis confirms the method's validity and alignment with current practices, noting improvements in study reproducibility over the past decade. They recommend using this approach for further reproducibility and validation investigations in software engineering research [12]. Fernández et al. discuss the Empirical Software Engineering journal's open science initiative, aimed at enhancing transparency, reproducibility, and collaboration in software engineering research. The initiative includes guidelines for authors, reviewers, and editors, and introduces an open access policy to improve accessibility, reusability, and reliability of research. It promotes sharing data and tools, employing rigorous methodologies, and engaging the community to advance knowledge in empirical software engineering [13]."
