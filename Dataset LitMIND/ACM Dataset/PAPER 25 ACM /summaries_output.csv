PDF File Name,Summary_T5,Summary_Gemini,Summary_Gpt4,Summary_Gpt3,Summary_BART
0,ad-hoc retrieval task is formalized as a matching problem between two pieces of text . ad-hoc retrieval task is mainly about relevance matching while most NLP tasks concern semantic matching .,"A deep relevance matching model (DRMM) is proposed for ad-hoc retrieval. DRMM addresses the unique characteristics of ad-hoc retrieval by handling exact matching signals, query term importance, and diverse matching requirements through a joint deep architecture at the query term level. Experiments demonstrate DRMM's significant outperformance over existing retrieval models and state-of-the-art deep matching models.","Despite the success of deep neural networks in various fields, their application in ad-hoc retrieval has been limited due to inadequate handling of key task characteristics. This paper introduces a novel deep relevance matching model (DRMM) that effectively addresses relevance through a specialized architecture, significantly outperforming existing models in benchmark tests.","Recent advancements in deep neural networks have revolutionized various fields, yet ad-hoc retrieval tasks have seen limited success with deep models. Addressing this gap, a novel Deep Relevance Matching Model (DRMM) is proposed, emphasizing relevance matching over semantic matching. By integrating matching histogram mapping and term gating network, the DRMM outperforms existing retrieval models.","A Deep Relevance Matching Model for Ad-hoc RetrievalABSTRACT MATCHING MACHINESIn recent years, deep neural networks have led to exciting breakthroughs in speech recognition, computer vision, and"
1,our linear attention transformers achieve similar performance to vanilla transformers and they are up to 4000x faster on autoregressive prediction of very long sequences .,"Fast Autoregressive Transformers reduce computational complexity from O(N^2) to O(N) by expressing attention as a linear dot-product of kernel feature maps. This allows for iterative implementation, accelerating autoregressive transformers and linking them to recurrent neural networks. Despite comparable performance to vanilla transformers, linear transformers exhibit significantly faster autoregressive prediction on long sequences, with a maximum speed improvement of 4000x.","The research introduces a linear attention mechanism for transformers, reducing computational complexity from O(N^2) to O(N) by utilizing kernel feature maps and matrix product associativity. This approach significantly speeds up autoregressive transformers—up to 4000 times faster—while maintaining performance, and highlights similarities with recurrent neural networks.","Linear Transformers are proposed to accelerate Autoregressive Transformers by expressing self-attention as a linear dot-product, reducing complexity from O(N^2) to O(N). This formulation enables an iterative implementation, significantly speeding up autoregressive prediction. Linear Transformers achieve comparable performance to vanilla transformers and are up to 4000x faster for long sequences.","Fast Autoregressive Transformers with Linear AttentionTransformers achieve remarkable performance in several tasks but due to their quadratic complexity, with respect to the input’s length, they are prohibitively slow for very long sequences. To"
2,"large transformer models can be expensive to train, especially on long sequences . we introduce two techniques to improve the efficiency of large transformer models . the reformer model performs on par with Transformer models while being much more memory-efficient","The Reformer is a Transformer model that uses locality-sensitive hashing attention and reversible residual layers to improve efficiency. These techniques reduce the computational complexity of the attention mechanism and allow for memory savings during training. As a result, the Reformer performs comparably to Transformer models but is more memory-efficient and faster on long sequences.",The Reformer introduces two innovations to enhance Transformer efficiency: locality-sensitive hashing to reduce attention complexity from O(L^2) to O(LlogL) and reversible residual layers that minimize memory usage during training. These advancements enable the Reformer to match traditional Transformer performance but with greater speed and memory efficiency on long sequences.,"""Reformer: The Efficient Transformer"" introduces two techniques to enhance Transformer efficiency. By utilizing locality-sensitive hashing and reversible residual layers, the model achieves state-of-the-art results while significantly reducing memory usage and speeding up processing on long sequences. This innovation addresses the high cost of training large Transformer models effectively.",REFORMER: THE EFFICIENT TRANSFORMERABSTRACT OF TRANSFORMERSLarge Transformer models routinely achieve state-of-the-art results on a number of tasks but training these models can be prohibitively costly
3,axiomatic attribution for deep networks is a problem previously studied . it is based on two fundamental axioms: sensitivity and implementation invariance . the method requires no modification to,"Axiomatic Attribution for Deep Networks proposes a new attribution method called Integrated Gradients that satisfies two fundamental axioms: Sensitivity and Implementation Invariance. Unlike existing methods, Integrated Gradients is simple to implement and requires no modifications to the original network. Experiments demonstrate its effectiveness in debugging, rule extraction, and improving user engagement with deep learning models across various domains, including image, text, and chemistry.","This research addresses the challenge of attributing deep network predictions to input features, identifying two key axioms—Sensitivity and Implementation Invariance—that many existing methods fail to meet. The study introduces a new, simple-to-implement method, Integrated Gradients, which effectively enhances model transparency and user interaction across various applications.","This study introduces two key axioms, Sensitivity and Implementation Invariance, for attributing deep network predictions to input features. Existing methods fail to meet these axioms, highlighting a critical weakness. The proposed Integrated Gradients method, guided by these axioms, is simple to implement and effective in debugging networks, extracting rules, and enhancing user interaction with models.","Axiomatic Attribution for Deep NetworksAbstract:We study the problem of attributing the prediction of a deep network to its input features, a problem previously studied by several other works. We identify two fundamental axioms—Sensitivity and"
4,bag-of-words features are a popular feature in machine learning . they lose the order of the words and ignore semantics of the words .,"Paragraph Vector, an unsupervised algorithm, addresses the limitations of bag-of-words text representation by preserving word order and considering word semantics. This algorithm creates fixed-length feature vectors for variable-length texts, enabling prediction of document words from the vector. Empirical evaluations demonstrate Paragraph Vector's superiority over bag-of-words and other text representation techniques, resulting in improved performance on tasks like text classification and sentiment analysis.","The paper introduces Paragraph Vector, an unsupervised algorithm that learns fixed-length feature representations from texts, addressing the limitations of bag-of-words models by preserving word order and capturing semantics. This method outperforms traditional models and achieves state-of-the-art results in text classification and sentiment analysis tasks.","The paper introduces Paragraph Vector, an innovative unsupervised algorithm that learns fixed-length feature representations from variable-length texts. Overcoming the limitations of bag-of-words models, it captures word semantics and order. Empirical evidence demonstrates its superiority in text representation, achieving state-of-the-art results in text classification and sentiment analysis tasks.","Distributed Representations of Sentences and DocumentsMany machine learning algorithms require the input to be represented as a fixed-length feature vector. When it comes to texts, one of the most common fixed- length features is bag-of-"
5,BIGBIRD is a sparse attention mechanism that reduces quadratic dependency on sequence length . it can handle sequences of length up to 8x of what was previously possible using similar hardware .,"BIGBIRD, a sparse attention mechanism, addresses the quadratic dependency of Transformers on sequence length, reducing it to linear. BIGBIRD approximates sequence functions universally and is Turing complete. It enables handling sequences up to 8x longer, boosting NLP tasks like question answering and summarization, and finds novel applications in genomics.","The proposed BIGBIRD model introduces a sparse attention mechanism to Transformers, significantly reducing memory dependency from quadratic to linear, enabling processing of sequences up to 8 times longer. This enhancement preserves full model capabilities, improves NLP task performance, and opens new applications in genomics, maintaining Turing completeness and universal approximation.","BIGBIRD, a sparse attention mechanism, addresses the quadratic dependency issue in Transformers models like BERT, enabling handling of longer sequences. It is a universal approximator of sequence functions, Turing complete, and significantly boosts NLP task performance. The proposed sparse attention extends sequence length capabilities by 8x, enhancing applications in genomics data.","Big Bird: Transformers for Longer SequencesTransformers-based models, such as BERT, have been one of the most successful deep learning models for NLP. Unfortunately, one of their core limitations is the quadratic dependency"
6,a key solution to QA systems is to retrieve the most similar question of a given question . most existing models for PI and NLI have at least two problems: they rely on a large amount of labeled,"Transfer learning is applied to paraphrase identification (PI) and natural language inference (NLI) for question-answering systems. The proposed framework simultaneously learns shared representations and domain relationships. An efficient hybrid model combines sentence encoding and interaction for improved performance. Experiments show that transfer learning significantly boosts performance, capturing insightful inter-domain and intra-domain relationships. The model's deployment in an online chatbot demonstrates practical improvements.","This paper explores transfer learning for paraphrase identification (PI) and natural language inference (NLI) in question-answering systems, proposing a framework that learns both shared representations and domain relationships. It introduces a hybrid model combining sentence encoding and interaction methods, demonstrating significant performance improvements in e-commerce applications and insights into domain relationships.","This paper explores transfer learning for paraphrase identification and natural language inference in question-answering systems. By simultaneously learning shared representations and domain relationships, a hybrid model is proposed for efficient adaptation of knowledge across domains. Experimental results show promising performance improvements, with the model successfully deployed in an online chatbot system.","Modelling Domain Relationships for Transfer Learning on Retrieval-based Question Answering Systems in E-commerceABSTRACT: Transfer Learning for Queries and Natural Language Indicators (PI and NLI)Nowadays, it"
7,a hierarchical attention network is proposed for document classification . the model has two levels of attention mechanisms applied at the word- and sentence-level .,"The proposed hierarchical attention network for document classification incorporates a hierarchical structure mirroring document structure and two attention levels at word- and sentence-level. This enables differential attention to important content during document representation construction. Experiments on six text classification tasks show significant performance improvements over previous methods. Attention layer visualization reveals the model's selection of relevant words and sentences, demonstrating its effectiveness.","The proposed hierarchical attention network enhances document classification by mirroring document structure and incorporating dual-level attention mechanisms at both word and sentence levels. This architecture significantly outperforms existing methods in extensive text classification tasks, with visualizations confirming its ability to identify key words and sentences effectively.","Proposing a novel hierarchical attention network for document classification, this model mirrors the document's structure and incorporates two levels of attention mechanisms at the word and sentence levels. Outperforming previous methods on large-scale text classification tasks, the model selectively attends to important content, as evidenced by visualization of attention layers selecting informative words and sentences.",Hierarchical Attention Networks for Document ClassificationWe propose a hierarchical attention network for document classification. Our model has two distinctive characteristics: (i) it has a hierarchical structure that closely mirrors the hierarchical structure of documents; (ii) it
8,paper develops neural summarization model which can process multiple documents . model can encode documents in a hierarchical manner . model learns latent dependencies among textual units .,"This paper presents a hierarchical Transformer model for multi-document summarization. The model effectively encodes multiple documents, capturing cross-document relationships through an attention mechanism. It incorporates both latent dependencies and explicit graph representations. Experimental results on the WikiSum dataset show significant improvements over baselines, demonstrating the model's effectiveness in abstractive multi-document summarization.","This paper introduces a neural summarization model that enhances the Transformer architecture to hierarchically encode multiple documents, utilizing an attention mechanism to represent cross-document relationships and process latent and explicit textual dependencies. Empirical tests on the WikiSum dataset show significant improvements over existing baselines.","This paper introduces a neural summarization model, Hierarchical Transformers, for multi-document summarization. The model encodes documents hierarchically and captures cross-document relationships using an attention mechanism. It outperforms strong baselines on the WikiSum dataset by learning latent dependencies and leveraging explicit graph representations for improved abstractive summaries.","Hierarchical Transformers for Multi-Document SummarizationIn this paper, we develop a neural summarization model which can effectively process multiple input documents and distill abstractive summaries. Our model augments a previously proposed Transformer"
9,sentence embeddings are an important component of many natural language processing (NLP) systems . the highest performing solutions for learning sentence embeddings require labelled data .,"DeCLUTR introduces an unsupervised method for learning sentence embeddings using deep contrastive learning. It eliminates the need for labeled training data, making it applicable to a wider range of languages and domains. By leveraging transformer-based language models and scaling with training data size, DeCLUTR achieves competitive performance to supervised methods without requiring labeled data.","DeCLUTR introduces a self-supervised method for learning universal sentence embeddings without labeled data, leveraging deep metric learning. This approach enhances transformer-based models, narrowing the performance gap with supervised methods. It scales with data and model size, offering publicly available code and models for broad adaptability and application.","DeCLUTR introduces Deep Contrastive Learning for Unsupervised Textual Representations, offering a self-supervised approach for learning universal sentence embeddings without the need for labelled data. By leveraging deep metric learning techniques, this method enhances transformer-based language models, bridging the performance gap between unsupervised and supervised pre-training for sentence encoders. The scalability of learned embeddings is demonstrated to improve with increased trainable parameters and unlabelled training data.","DeCLUTR: Deep Contrastive Learning for Unsupervised Textual RepresentationsSentence embeddings are an important component of many natural language processing (NLP) systems. Like word embeddentings, sentence embedd"
10,siamese multi-depth transformer-based hierarchical encoder for long-form document matching . model can increase maximum input text length from 512 to 2048 .,"The Siamese Multi-depth Transformer-based Hierarchical Encoder (SMITH) model is proposed for long-form document matching. SMITH addresses the computational limitations of self-attention models for long text by incorporating a hierarchical encoder and masked sentence block language modeling pre-training. The model achieves superior performance on benchmark datasets, expanding the maximum input text length limit from 512 to 2048 tokens. This opens up new possibilities for research and applications in long-form document matching tasks such as news recommendation and document clustering.","The paper introduces the Siamese Multi-depth Transformer-based Hierarchical (SMITH) Encoder, designed to enhance semantic matching between long-form documents, a less explored area with applications in recommendations and clustering. The SMITH model surpasses previous models by extending input text length and incorporating novel pre-training tasks, showing superior performance on benchmark datasets.","The study introduces the Siamese Multi-depth Transformer-based Hierarchical (SMITH) Encoder to address the limitations of existing models in long-form document matching. By incorporating a transformer-based hierarchical encoder and a novel masked sentence block language modeling task, SMITH outperforms previous state-of-the-art models, extending the input text length from 512 to 2048 tokens.",Beyond 512 Tokens: Siamese Multi-depth Transformer-based Hierarchical Encoder for Long-Form Document MatchingABSTRACT: The task of semantic matching between long-form documents is relatively less explored and needs more
11,a new framework is being developed to help interpret long-form documents . the framework is based on supervised contrastive learning . the code for the proposed model is publicly available at https://github.com/InterDigitalIn,"CoLDE, a transformer-based framework, addresses challenges in matching long-form documents by using unique embeddings, a multi-headed attention layer, and supervised contrastive learning. It captures similarity at three levels: document pairs, document sections, and document chunks. Evaluated on three datasets, CoLDE outperforms existing methods, is robust to varying document lengths and perturbations, and provides interpretable results, aiding in understanding document similarity. The source code is publicly available.","Recent advancements in semantic text matching struggle with long-form documents due to inherent challenges like varying contexts and partial similarities. The CoLDE framework, a transformer-based model with unique embeddings and multi-headed attention, effectively captures nuanced document similarities at multiple levels, offering superior performance and interpretability on diverse datasets.","""Supervised Contrastive Learning for Interpretable Long-Form Document Matching introduces CoLDE, a transformer-based framework addressing challenges in comparing long documents. CoLDE captures similarity at multiple levels using unique embeddings and a contrastive learning framework, outperforming existing methods on various datasets. It offers interpretability and robustness, with code available for public use.""","Supervised Contrastive Learning for Interpretable Long-Form Document MatchingRecent advancements in deep learning techniques have transformed the area of semantic text matching (STM). However, most state-of-the-art models are designed to operate"
12,"a novel document ranking model composed of two separate deep neural networks is proposed . one matches the query and the document using a local representation, and another matches the query and the document using learned distributed representations .","The proposed model combines local and distributed text representations to enhance web search ranking. It comprises two neural networks that perform matching using both representations, jointly trained as part of a single network. Experiments demonstrate that this duet model outperforms individual networks and traditional models, indicating the complementary benefits of combining local and distributed representations for document ranking.","The abstract discusses a novel document ranking model that integrates both local and distributed text representations through two deep neural networks, enhancing web search effectiveness. This dual approach, termed a 'duet', outperforms traditional and recent neural network-based models in web page ranking tasks.","This study explores the synergy between local and distributed text representations for web search. By combining traditional local matching with neural network-based distributed matching, a novel document ranking model is proposed. The 'duet' model outperforms individual neural networks and traditional baselines, showing promise for improving web page ranking tasks.",Learning to Match Using Local and Distributed Representations of Text for Web SearchABSTRACT: A novel document ranking model for Web page ranking.Models such as latent semantic analysis and those based on neural embeddings learn distributed representations
13,attention mechanisms play a central role in recurrent neural network (RNN) models . recent paper claims that ‘attention is not Explanation’ .,"Attention mechanisms facilitate the understanding of RNN models, but the efficacy of using them for explanation is disputed. This paper challenges the claim that attention is not an explanatory mechanism and proposes alternative tests to evaluate its explanatory power. These tests assess uniform weight distributions, variance calibration, diagnostic frameworks, and adversarial attention training. The authors demonstrate that adversarial distributions do not disprove the utility of attention for explanation.","The debate on whether attention mechanisms in recurrent neural networks (RNNs) provide explanations for model decisions is contentious. A recent critique, ""Attention is not Explanation,"" is challenged by proposing tests to evaluate attention's explanatory power, including uniform-weights baselines and adversarial training. These tests suggest attention's potential in meaningful model interpretation, despite some adversarial limitations.","Attention mechanisms are crucial in NLP systems, particularly in RNN models. A recent study challenges the notion that attention equals explanation, proposing alternative tests to assess attention's explanatory power. These tests include uniform-weights baseline, variance calibration, diagnostic framework, and adversarial attention training. Results suggest attention mechanisms remain valuable for explainability in RNN models.","Attention is not not ExplanationAttention mechanisms play a central role in NLP systems, especially within recurrent neural network (RNN) models. Recently, there has been increasing interest in whether or not the intermediate representations offered by these"
14,text matching is a fundamental problem in many natural language processing tasks . convolutional neural network can capture many complicated patterns . text matching is a problem of image recognition .,"Text matching is formulated as image recognition, where a matching matrix is constructed as an ""image"" and a convolutional neural network extracts matching patterns. This approach effectively captures n-gram and n-term matchings, outperforming baselines in text matching tasks.","The research proposes modeling text matching as image recognition, using a convolutional neural network (CNN) to analyze a matrix representing word similarities. This approach, inspired by CNN success in image pattern recognition, effectively identifies complex text patterns, showing superior results in experiments compared to baseline methods.","Text Matching is crucial in natural language processing. Drawing inspiration from convolutional neural networks in image recognition, the proposed approach models text matching as an image recognition problem. By constructing a matching matrix and utilizing a convolutional neural network, the model successfully identifies salient signals like n-gram and n-term matchings, outperforming baselines.","Text Matching as Image RecognitionMatching two texts is a fundamental problem in many natural language processing tasks. An effective way is to extract meaningful matching patterns from words, phrases, and sentences to produce the matching score. Inspired by the"
15,"our method returns an “explanation” consisting of groups of input-output tokens that are causally related . we focus the general approach on sequence-to-sequence problems, adopting a variational",This study presents a method for interpreting black-box sequence-to-sequence models by identifying causal relationships between input and output tokens. The approach uses input perturbations and a graph-based analysis to infer dependencies. Testing on NLP sequence generation tasks shows that the method provides meaningful explanations for model predictions.,"This research introduces a method to explain predictions from black-box sequence-to-sequence models by analyzing causally related groups of input-output tokens. The approach involves perturbing inputs, generating response graphs, and solving a partitioning problem to identify key components, tested on various NLP tasks using a variational autoencoder for input perturbations.","This study proposes a causal framework to interpret black-box sequence-to-sequence model predictions. By perturbing inputs and analyzing responses, causal relationships between input-output tokens are identified. The method utilizes a variational autoencoder for meaningful perturbations and is validated on various NLP tasks.",A causal framework for explaining the predictions of black-box sequence-to-sequence modelsAbstract: We use a causal framework to explain the prediction of blackbox sequence to sequence problems in NLP.We interpret the predictions for any black-
16,"aNMM model can outperform other neural network models . it is combined with additional features such as word overlap or BM25 scores . without this combination, these models perform significantly worse than methods based on linguistic feature","aNMM is an attention-based neural matching model for ranking short answer text that uses value-shared weighting and question term importance learning. In experiments using TREC QA data, aNMM outperforms other neural network models for question answering and is competitive with models that use additional features. When combined with additional features, aNMM outperforms all baselines.","The paper introduces aNMM, an attention-based neural matching model for ranking short answer texts, which outperforms existing neural network models on the TREC QA benchmark. Unlike previous models, aNMM effectively integrates question term importance and adopts a unique value-shared weighting scheme, enhancing performance even further when combined with additional features.","This paper introduces an attention-based neural matching model (aNMM) for ranking short answer texts, outperforming other neural network models for question answering. By incorporating a value-shared weighting scheme and question term importance learning, aNMM competes with models using additional features and surpasses all baselines when combined with them.","aNMM: Ranking Short Answer Texts with Attention-Based Neural Matching ModelABSTRACT-based neural matching modelAs an alternative to question answering methods based on feature engineering, deep learning approaches such as convolutional neural networks"
17,Poolingformer aggregates information from neighbors in a smaller window pattern . it's second level uses a larger window to increase receptive fields with pooling attention . Experimental results show that Poolingformer sit,"Poolingformer, a novel two-level attention mechanism, excels in long document modeling. It leverages a sliding window pattern for local information aggregation and pooling attention for wider receptive fields, resulting in reduced computational costs and memory consumption. Poolingformer achieves state-of-the-art results on long sequence QA tasks (NQ and TyDi QA) and demonstrates exceptional performance on long sequence summarization (arXiv).","The paper introduces Poolingformer, a novel two-level attention model for long document processing, enhancing computational efficiency and reducing memory use. It outperforms existing models in long sequence QA tasks and summarization, leading three official leaderboards and demonstrating superior performance on the arXiv benchmark.","""Poolingformer introduces a two-level attention schema for long document modeling, enhancing information aggregation and reducing computational costs. Evaluation on QA tasks and summarization benchmarks shows significant performance improvements, outperforming previous models on multiple leaderboards. Experimental results confirm Poolingformer's superior performance in long sequence processing.""","Poolingformer: Long Document Modeling with Pooling AttentionIn this paper, we introduce a two-level attention schema, Poolingformer, for long document modeling. Its first level uses a smaller sliding window pattern to aggregate information from"
18,deep learning models have achieved remarkable success in natural language inference (NLI) tasks . it is often unclear how and why they actually work .,"Deep learning models for natural language inference (NLI) are difficult to interpret, hindering understanding of their reasoning processes. This study visualizes attention and LSTM gating signals in an NLI model to provide insights into its decisions. By analyzing the saliency of these signals, researchers identify critical information contributing to model predictions, revealing the strengths and limitations of the model's reasoning capabilities.","This paper explores the interpretability of deep learning models in natural language inference (NLI) by visualizing attention and LSTM gating signals in a case study. The proposed method reveals critical insights and identifies key information influencing model decisions, enhancing understanding of how these models function.","This study delves into interpreting complex neural models for natural language inference, aiming to shed light on their inner workings. By visualizing attention and LSTM gating signals, the researchers uncover key insights and clarify the decision-making process. Despite the challenges of interpreting deep learning models, this approach offers valuable explanations for their success.","Interpreting Recurrent and Attention-Based Neural Models: a Case Study on Natural Language InferenceDeep learning models have achieved remarkable success in natural language inference (NLI) tasks. While these models are widely explored, they are hard"
19,document vector through corruption (Doc2VecC) is an efficient document representation learning framework . it represents each document as a simple average of word embeddings . the corruption model introduces a data-dependent,"Doc2VecC efficiently represents documents as word embedding averages, regularized by a corruption model that emphasizes informative words. Its simplicity allows for fast training and inference while outperforming state-of-the-art methods in sentiment analysis, document classification, and semantic relatedness tasks.","The Document Vector through Corruption (Doc2VecC) framework efficiently learns document representations by averaging word embeddings and incorporating a corruption model that emphasizes informative words while diminishing common ones. Doc2VecC outperforms traditional models in tasks like sentiment analysis and document classification, offering rapid training and effective unseen document representation.","""Doc2VecC introduces an efficient document representation learning framework by averaging word embeddings, incorporating a corruption model for regularization. It outperforms Word2Vec, excelling in sentiment analysis, document classification, and semantic tasks. The model's simplicity allows for fast training on large datasets and efficient generation of document representations at test time.""","EFFICIENT VECTOR REPRESENTATION FOR DOCUMENTS THROUGH CORRUPTIONABSTRACT OF THE DOCUMENT RULESWe present an efficient document representation learning framework, Document Vector through Corruption (Doc2"
20,Performers can estimate regular (softmax) full-rank-attention Transformers . architectures use a novel Fast Attention Via positive Orthogonal Random features approach . performers can also model kernelizable attention mechanisms beyond,"Performers are novel Transformer architectures that can accurately estimate full-rank-attention Transformers with linear time and space complexity. They use a Fast Attention Via positive Orthogonal Random features (FAVOR+) approach to approximate softmax attention kernels, enabling efficient modeling of various attention mechanisms. Performers demonstrate competitive performance on diverse tasks, showcasing the effectiveness of their attention-learning paradigm.","The paper introduces Performers, a Transformer architecture that efficiently approximates full-rank softmax attention using a novel method called FAVOR+, which operates in linear time and space. Performers offer strong theoretical guarantees and demonstrate competitive performance across various tasks, including text and protein sequence modeling, without requiring sparsity or low-rank assumptions.","Performers are innovative Transformer architectures that estimate attention with linear complexity, challenging traditional softmax methods. Using Fast Attention Via positive Orthogonal Random features (FAVOR+), Performers offer scalable kernel methods and efficient attention modeling. With strong theoretical guarantees and competitive performance across diverse tasks, Performers redefine attention-learning paradigms for large-scale applications.","RETHINKING ATTENTION WITH PERFORMERSABSTRACT PERFORMERSWe introduce Performers, Transformer architectures which can estimate regular (softmax) full-rank-attention Transformers with provable accuracy, but using"
21,a new method for explaining differentiable models is introduced . it uses a method that penalizes their input gradients .,"A method that enhances neural network models by leveraging their explanations is proposed. It identifies and penalizes gradients to improve model accuracy and reliability. The approach applies both expert annotations and unsupervised learning to generate multiple classifiers with distinct decision boundaries. On various datasets, it demonstrates better generalization capabilities, particularly when training and testing conditions differ.","The paper introduces a method to enhance trust in neural networks by constraining their explanations, addressing their opacity in critical applications. By examining and penalizing input gradients, the method not only provides scalable, faithful explanations but also improves model generalization across varying conditions in training and testing environments.","This study addresses the challenge of trusting neural networks due to their opacity, especially in critical applications. Existing tools like LIME help reveal implicit rules behind predictions, but are limited in scalability and corrective capabilities. The proposed method efficiently explains and regularizes differentiable models by penalizing input gradients, leading to improved generalization in varying conditions.","Right for the Right Reasons: Training Differentiable Models by Constraining their ExplanationsNeural networks are among the most accurate supervised learning methods in use today. However, their opacity makes them difficult to trust in critical applications, especially if"
22,document classification using BERT is the first application of the tool . document classification is a complex task that requires a lot of computational effort .,"DocBERT, an adaptation of BERT, effectively tackles document classification despite document-specific challenges. Its straightforward application outperforms existing methods across four datasets. Knowledge distillation reduces computational costs by transferring knowledge from BERT large to smaller LSTMs, matching the performance of BERT base with significantly fewer parameters. DocBERT establishes strong baselines for further research in document classification.","The paper introduces DocBERT, the first use of BERT for document classification, achieving state-of-the-art results on four datasets despite initial concerns about BERT's suitability for this task. It also presents a method to reduce BERT's computational demands by distilling its knowledge into smaller models, maintaining performance with significantly fewer parameters.","""DocBERT pioneers BERT for document classification, defying initial skepticism. Despite challenges like lengthy documents and multiple labels, a simple BERT-based model excels, setting new benchmarks on key datasets. By distilling BERT knowledge into compact LSTMs, computational efficiency is enhanced without compromising performance. This work establishes enhanced baselines for future advancements.""","DocBERT: BERT for Document ClassificationAbstractWe present, to our knowledge, the first application of BERT to document classification. A few characteristics of the task might lead one to think that BERT is not the most appropriate model"
23,CogLTX is a cognitive theory based on Baddeley's theory . it identifies key sentences by training a judge model and concatenates them for reasoning .,"CogLTX, inspired by human cognition, addresses BERT's limitations in handling long texts. It identifies key sentences, concatenates them for reasoning, and enables multi-step reasoning through rehearsal and decay. Using intervention-based supervision, CogLTX outperforms or matches state-of-the-art models on downstream tasks, with memory overhead independent of text length.","CogLTX addresses BERT's limitations in processing long texts by mimicking human cognitive strategies based on Baddeley's theory. It selects key sentences using a judge model, enabling multi-step reasoning with minimal memory overhead. This approach achieves competitive results on various tasks without requiring relevance annotations, using interventions for supervision.","CogLTX addresses BERT's limitations with long texts by identifying key sentences, concatenating them for reasoning, and enabling multi-step reasoning. Inspired by human cognitive theory, it outperforms state-of-the-art models on various tasks with minimal memory overheads, offering a novel approach to processing long texts efficiently.","CogLTX: Applying BERT to Long TextsBERT is incapable of processing long texts due to its quadratically increasing memory and time consumption. The most natural ways to address this problem, such as slicing the text"
24,supervised contrastive learning has seen a resurgence in recent years . the self-supervised batch contrastive approach subsumes or significantly outperforms traditional contrastive losses .,"Supervised contrastive learning (SupCon) extends self-supervised contrastive learning to supervised settings by pulling together points belonging to the same class and pushing apart those from different classes. On ImageNet, SupCon achieves top-1 accuracy of 81.4% with ResNet-200, outperforming cross-entropy loss. SupCon offers robustness to natural corruptions and stability to hyperparameter settings.","Supervised Contrastive Learning enhances self-supervised batch contrastive approaches by utilizing label information to cluster same-class points together and separate different-class clusters in embedding space. This method achieves a top-1 accuracy of 81.4% on ImageNet with ResNet-200, outperforming traditional losses and demonstrating robustness and stability across various settings.","""Supervised Contrastive Learning enhances deep image model training by leveraging label information. The approach pulls together same-class points and pushes apart different-class samples in embedding space. Achieving 81.4% accuracy on ImageNet, it outperforms cross-entropy on various datasets and ResNet variants. The loss offers robustness and stability, with simple implementation.""","Supervised Contrastive LearningAbstractContrastive learning applied to self-supervised representation learning has seen a resurgence in recent years, leading to state of the art performance in the unsupervised training of deep image models. Modern batch contrast"
25,a new paper shows how sequence models can generate long sequences . the paper introduces sparse factorizations of the attention matrix .,"Sparse Transformers address scalability limitations of transformer models for long sequences, offering a quadratic reduction in time and memory requirements. With architecture enhancements, memory optimization, and fast attention kernels, Sparse Transformers enable the training of deep networks on sequences of tens of thousands of timesteps. Used for modeling images, audio, and text, Sparse Transformers set new density modeling state-of-the-art results for various datasets. They demonstrate global coherence and high diversity in unconditional samples, showcasing the potential for self-attention in modeling sequences of unprecedented length.","This paper introduces Sparse Transformers, which utilize sparse factorizations of the attention matrix to efficiently handle long sequences, reducing computational requirements from quadratic to linear. Enhancements include deeper network training, memory-efficient recomputation, and fast attention kernels. These innovations enable modeling of extensive sequences across different data types, achieving state-of-the-art results in density modeling for Enwik8, CIFAR-10, and ImageNet-64, and demonstrating potential for handling sequences up to one million timesteps.","This paper introduces Sparse Transformers, reducing time and memory complexity to O(npn) by sparse factorizations of attention matrices. It presents architectural variations, memory-saving techniques, and fast attention kernels for training deep networks. Sparse Transformers model long sequences effectively, achieving state-of-the-art results in various tasks and demonstrating potential for modeling sequences of over one million timesteps.","Generating Long Sequences with Sparse TransformersTransformers are powerful sequence models, but require time and memory that grows quadratically with the sequence length. In this paper we introduce sparse factorizations of the attention matrix which reduce this to"
26,a new method uses contrastive augmentation to learn document representations . the method can reduce the classification error rate by up to 6.4% .,"This study introduces a method for learning document representations unsupervised using contrastive learning and data augmentation. The approach leverages paraphrased documents to create embeddings invariant to semantic changes. Findings demonstrate the benefits of contrastive augmentation for document representation learning, with word-level augmentations outperforming sentence-level and document-level ones. When used in classification, this method outperforms state-of-the-art baselines and even rivals fully-supervised methods.","The study introduces an unsupervised method for learning document embeddings using contrastive learning and data augmentation, inspired by self-supervised techniques in image and NLP fields. It demonstrates that contrastive augmentation significantly enhances document representation quality, particularly with stochastic, word-level manipulations, and reduces classification error rates by up to 6.4% compared to state-of-the-art methods.","This study introduces a contrasting learning approach with data augmentation for unsupervised document embedding. By leveraging contrastive self-supervised learning techniques, the research demonstrates significant improvements in document representation quality. The method outperforms state-of-the-art approaches, reducing classification errors by up to 6.4% on benchmark datasets, rivaling fully-supervised methods.",Unsupervised Document Embedding via Contrastive AugmentationWe present a contrasting learning approach with data augmentation techniques to learn document representations in an unsupervised manner. Inspired by recent contrastive self-supervised learning algorithms used for
27,"simple contrastive learning framework that greatly advances state-of-the-art sentence embeddings . paper presents SimCSE, which takes an input sentence and predicts itself in a contrastive objective .","SimCSE introduces a simple contrastive learning framework for sentence embedding that outperforms existing methods. By predicting the original sentence using contrastive objectives and utilizing dropout for data augmentation, the unsupervised approach achieves results comparable to supervised methods. A supervised approach incorporating annotated pairs from natural language inference datasets further enhances performance. SimCSE regularizes the embeddings' anisotropic space and aligns positive pairs effectively, leading to improved semantic textual similarity task performance.","The paper introduces SimCSE, a contrastive learning framework for enhancing sentence embeddings. It describes an unsupervised method using self-prediction with dropout, and a supervised method leveraging natural language inference pairs. SimCSE significantly improves semantic textual similarity scores, achieving new state-of-the-art results, and regularizes embeddings' space effectively.","SimCSE introduces a simple contrastive learning framework for sentence embeddings, achieving state-of-the-art performance. The unsupervised method predicts sentences in a contrastive objective with dropout as noise, performing comparably to supervised approaches. Incorporating natural language inference data improves results, with BERT models achieving significant Spearman's correlation improvements. The framework regularizes embeddings and aligns positive pairs effectively.","SimCSE: Simple Contrastive Learning of Sentence EmbeddingsThis paper presents SimCSE, a simple contrastive learning framework that greatly advances state-of-the-art sentence embeddings. We first describe an un"
28,a modified version of the LSTM learning algorithm is presented . bidirectional LSTM outperforms unidirectional ones in phoneme classification .,"Bidirectional Long Short Term Memory (BLSTM) networks outperform unidirectional architectures in framewise phoneme classification. LSTM is faster and more accurate than standard Recurrent Neural Nets (RNNs) and time-windowed Multilayer Perceptrons (MLPs). Contextual information plays a crucial role in speech processing, and BLSTM effectively exploits this information.","This paper introduces and evaluates bidirectional Long Short Term Memory (LSTM) networks for phoneme classification using the TIMIT database. Findings indicate that bidirectional networks surpass unidirectional ones in performance, with LSTM proving faster and more accurate than standard RNNs and MLPs, highlighting the importance of contextual information in speech processing.","Bidirectional LSTM networks, including BLSTM, outperform unidirectional ones in framewise phoneme classification. LSTM is faster and more accurate than RNNs and MLPs. Contextual information is vital in speech processing, with BLSTM proving to be an effective architecture for exploiting it. The study highlights the significance of bidirectional networks in improving classification tasks.","Framewise phoneme classification with bidirectional LSTM and other neural network architecturesIn this paper, we present bidirectionial Long Short Term Memory (LSTM) networks, and a modified, full gradient version of the"
29,"convolutional neural network architectures for matching natural language sentences . proposed models capture rich matching patterns at different levels . models are generic, requiring no prior knowledge on language .","Convolutional neural network (CNN) models are proposed for sentence matching by adapting the convolutional strategy used in computer vision and speech. These models effectively represent sentence structures and capture matching patterns at different levels. They are generic, requiring no language-specific knowledge, and applicable to various matching tasks across languages. Empirical evaluations show that the proposed models outperform competitor models in different matching tasks.","This text introduces convolutional neural network (CNN) models for matching natural language sentences, emphasizing their ability to model sentence structures and interactions effectively. These generic models, adaptable across languages and tasks, outperform competitors, as demonstrated in empirical studies on diverse matching tasks.","Proposing convolutional neural network models for sentence matching, this research emphasizes the importance of semantic matching in natural language tasks. The models effectively capture hierarchical structures and rich matching patterns, showcasing superior performance across various tasks without requiring prior language knowledge. Empirical studies validate the efficacy and versatility of the proposed approach.",Convolutional Neural Network Architectures for Matching Natural Language SentencesSemantic matching is of central importance to many natural language tasks. A successful matching algorithm needs to adequately model the internal structures of language objects and the interaction between them
30,simCLR: a simple framework for contrastive learning of visual representations . a linear classifier trained on self-supervised representations achieves 76.5% top-1 accuracy .,"SimCLR simplifies existing contrastive learning by utilizing data augmentation composition, introducing a non-linear transformation in the contrastive loss, and using larger batch sizes and training steps. These enhancements significantly improve the learned visual representations, allowing self-supervised models to match supervised learning performance on ImageNet. With 1% labeled data, SimCLR even surpasses supervised AlexNet, demonstrating its effectiveness in semi-supervised learning.","The paper introduces SimCLR, a simplified contrastive learning framework for visual representations, eliminating the need for specialized architectures or memory banks. It highlights the importance of data augmentation composition, a learnable nonlinear transformation, and larger batch sizes in enhancing the quality of learned representations. SimCLR significantly surpasses previous self-supervised and semi-supervised methods, achieving competitive accuracies on ImageNet.","This paper introduces SimCLR, a straightforward framework for contrastive learning of visual representations. It simplifies existing methods, emphasizing the importance of data augmentations, learnable transformations, batch sizes, and training steps. SimCLR surpasses previous self-supervised learning approaches, achieving remarkable accuracy on ImageNet with minimal labeled data, matching supervised models' performance.",A Simple Framework for Contrastive Learning of Visual RepresentationsThis paper presents SimCLR: a simple framework for contrastive learning of visual representations. We simplify recently proposed contrastive self-supervised learning algorithms without requiring specialized architectures or a
31,"axial attention in multidimensional data is a self-attention-based autoregressive model . the model is based on axial attention, a simple generalization of self-attention . the model is open","Axial Transformers, an autoregressive model for high-dimensional data, introduce axial attention to efficiently capture multidimensional dependencies. This allows for expressive distribution modeling while maintaining ease of implementation. The semi-parallel decoding structure enables fast generation even from large models, making Axial Transformers widely applicable. It achieves state-of-the-art results on image and video generation benchmarks.","We introduce Axial Transformers, a novel autoregressive model utilizing axial attention for efficiently handling high-dimensional tensor data. This architecture achieves state-of-the-art results on key benchmarks like ImageNet and BAIR Robotic Pushing, offering full distribution expressiveness and ease of implementation while optimizing memory and computational demands. The model's semi-parallel decoding structure enhances scalability and performance.","Axial Transformers introduce a self-attention-based autoregressive model for high-dimensional data, ensuring full expressiveness and ease of implementation while maintaining reasonable resource requirements. The axial attention mechanism aligns with the tensor dimensions, enabling parallel computation during decoding without independence assumptions. This approach achieves state-of-the-art results on various benchmarks and is open-sourced for wider adoption.","Axial Attention in Multidimensional TransformersABSTRACT-Based Autoregressive Model for Image and Other DataWe propose Axial Transformers, a self-attention-based autoregressive model for images and other data organized as high"
