Related_Work
"This research paper evaluates the quality of annotations for Turkish, Indonesian, and Minangkabau NLP tasks, comparing human-generated annotations to those from Large Language Models (LLMs). It reveals that while LLMs perform well in sentiment analysis, human annotations excel in complex tasks, highlighting LLM limitations in context understanding and ambiguity.Ostyakova et al. introduce a semi-automated method using Large Language Models for annotating open-domain conversations, comparing the efficacy of experts, crowdsourcing, and ChatGPT. They demonstrate that with tailored prompts, LLMs can achieve human-like performance in complex discourse analysis [1]. Zhu et al. investigated ChatGPT's ability to replicate human-generated labels in social computing tasks like stance detection and sentiment analysis. Results indicate potential with varying accuracy, highlighting areas for future research and improvement in data annotation tasks [2]. Alizadeh et al. investigated open-source Large Language Models (LLMs) like HugginChat and FLAN, comparing their performance in text annotation tasks to ChatGPT and MTurk. Findings indicate that these open-source LLMs surpass MTurk and are competitive with ChatGPT, offering advantages in cost-effectiveness and data security [3]. Gilardia et al. discussed how ChatGPT outperforms crowd workers in text annotation tasks, achieving higher accuracy and intercoder agreement at a significantly lower cost, thirty times cheaper than MTurk, using four datasets [4]. Belal et al. discuss ChatGPT, an AI product by OpenAI, which excels in sentiment analysis, outperforming lexicon-based methods with accuracy improvements of 20% and 25% on tweets and Amazon reviews, respectively, demonstrating its effectiveness as a data labeling tool [5]. Reiss et al. investigate ChatGPT's zero-shot capabilities for text annotation, highlighting its nondeterministic nature which leads to inconsistent outputs. They advise caution and emphasize the need for thorough validation against human-annotated data [6]. Wang et al. investigated GPT-3 as a cost-effective data labeler for NLP tasks, achieving 50%-96% cost reduction compared to human labeling. They proposed a framework combining GPT-3 and human labels to enhance performance within limited budgets [7]. Kuzman & Ljubešić discuss Automatic Genre Identification (AGI), a crucial text classification task for linguistics and natural language processing. Their survey covers genre schema definition, dataset collection, and machine learning strategies, emphasizing the need for a stable multilingual genre classifier and addressing dataset variability and methodology advancements [8]. Nasution & Onan's research compares human-generated and LLM-generated annotations in Turkish, Indonesian, and Minangkabau NLP tasks. Human annotations outperformed LLMs in complex tasks, though LLMs were competitive in sentiment analysis, underscoring the need for enhanced LLM capabilities [9]. Vujinović et al. investigate ChatGPT's in-context learning for annotating intelligent tutoring systems datasets, demonstrating a cost-effective alternative to human annotation with a new methodology and a publicly available dataset [10]. Koptyra et al. investigated the use of ChatGPT for text annotation and synthetic text generation, comparing three text collections: CLARIN-Emo, Stockbrief-GPT, and ChatGPT-Emo. They found that manual annotations by humans provide higher-quality data for personalized models [11]. Laskar et al. introduced CQSumDP, a ChatGPT-annotated resource for query-focused abstractive summarization, enhancing Debatepedia by regenerating queries to improve summary quality and query relevance. Evaluations confirm its superiority over the original dataset [12]. Ostyakova et al. investigate the taxonomy of Speech Functions, focusing on pragmatics, turn-taking, feedback, and topic switching. They utilize ChatGPT to create synthetic datasets for linguistic annotation, comparing expert and crowdsourced annotations, and demonstrating the tool's effectiveness in discourse tasks [13]. In a study, Törnberg et al. found that ChatGPT-4 outperforms both experts and crowd workers in accurately classifying political affiliations from Twitter messages using zero-shot learning, showing higher accuracy and reliability with equal or reduced bias [14]. Ollion et al. discuss the mixed performance of zero- or few-shot classifiers like ChatGPT in text annotation tasks, highlighting concerns about reproducibility, privacy, copyright, and language biases. Systematic reviews show these models often underperform compared to human-annotated ones [15]."
