PDF File Name,Summary_T5,Summary_Gemini,Summary_Gpt4,Summary_Gpt3,Summary_BART
0,crowdsourcing workers and ChatGPT are used to annotate open-domain conversations . guidelines include simple questions about the topic and speaker change .,"This paper introduces a semi-automated method for annotating open-domain conversations with speech functions using Large Language Models (LLMs) and hierarchical guidelines. The method allows for efficient and cost-effective annotation compared to traditional expert or crowdsourcing approaches. Experiments show that, with appropriate prompt engineering, LLMs can achieve human-like performance in certain annotation tasks, demonstrating their potential for complex discourse analysis.","This paper introduces a semi-automated method using Large Language Models for annotating open-domain conversations with speech functions, comparing the efficacy of experts, crowdsourcing, and ChatGPT, and demonstrating that with tailored prompts, ChatGPT can rival human annotators.","This paper introduces a semi-automated method for annotating open-domain conversations with speech functions using Large Language Models. Comparing expert, crowdsourcing, and ChatGPT annotations, it shows that large language models can achieve human-like performance in complex discourse annotation with prompt engineering techniques.","""ChatGPT vs. Crowdsourcing vs. Experts: Annotating Open-Domain Conversations with Speech Functions""This paper deals with the task of annotating open-domain"
1,chatGPT has uncovered possibilities whereby large language models can substitute human intelligence . the ability to reproduce human-generated label annotations could reduce cost and complexity of social computing research .,"ChatGPT demonstrates the ability to reproduce human-generated labels in social computing tasks, such as stance detection, sentiment analysis, hate speech, and bot detection. However, its performance varies across individual labels, and there is room for improvement. Further exploration can lead to new analysis methods and advancements in utilizing ChatGPT for human annotation tasks.","This study explores whether ChatGPT can replicate human-generated labels in social computing tasks, using it to re-label datasets in stance detection, sentiment analysis, hate speech, and bot detection. Results show potential, with varying accuracy across tasks, suggesting avenues for future research.","This study explores ChatGPT's ability to replicate human-generated labels in social computing tasks. Results show ChatGPT can handle data annotation tasks with an average accuracy of 0.609, excelling in sentiment analysis. Challenges persist, but this research paves the way for leveraging ChatGPT in human annotation tasks.",Title: Can ChatGPT Reproduce Human-Generated Labels? A Study of Social Computing TasksThe release of ChatGrypt has uncovered a range of possibilities whereby large language
2,open-source large language models outperform proprietary models like ChatGPT . study compares it with open-source models like HugginChat and FLAN .,"Open-source LLMs like HuggingChat and FLAN show strong performance in text annotation tasks, rivaling MTurk and even approaching ChatGPT's capabilities.","This study evaluates open-source Large Language Models (LLMs) like HugginChat and FLAN in text annotation, comparing them to ChatGPT and MTurk. Findings reveal that open-source LLMs surpass MTurk and rival ChatGPT in certain tasks, highlighting their cost-effectiveness and data security.","Open-source Large Language Models (LLMs) like HugginChat and FLAN surpass human-based services and rival ChatGPT in text annotation tasks. Despite ChatGPT's overall superior performance, open-source LLMs offer cost-effectiveness, transparency, and data protection, showing competitive potential in specific tasks.","""OPEN-SOURCE LARGE LANGUAGE MODELS OUTPERFORM CROWD WORKERS AND APPROACH CHATGPT IN TEXT-ANNOTATION TAS"
3,chatGPT outperforms crowd workers for text annotation tasks . per-annotation cost of ChatGPT is less than $0.003—about thirty times cheaper than MTur,"ChatGPT surpasses crowd workers in text annotation tasks, achieving higher accuracy and intercoder agreement. Its zero-shot performance exceeds crowd workers by 25 percentage points, and its annotation cost is 30 times cheaper than MTurk, demonstrating the potential of large language models in enhancing text classification efficiency.","Using four datasets (n=6,183), ChatGPT significantly outperformed crowd workers in text annotation tasks like relevance and topic detection, achieving higher accuracy and intercoder agreement at a cost thirty times lower than MTurk.","ChatGPT surpasses crowd workers in text annotation tasks, showing superior performance in relevance, stance, topics, and frame detection. With zero-shot accuracy exceeding crowd workers by 25%, ChatGPT also boasts higher intercoder agreement and significantly lower cost, highlighting its efficiency in text classification.","ChatGPT outperforms crowd workers for text-annotation tasksMany NLP applications require manual text annotations for a variety of tasks, notably to train classifiers or evaluate the performance of"
4,chatGPT is a new product of OpenAI and has emerged as the most popular AI product . it can answer questions on various topics and tasks .,"ChatGPT, an advanced language model, demonstrates remarkable performance in text annotation for sentiment analysis tasks. It significantly outperforms lexicon-based unsupervised methods, achieving an accuracy improvement of 20% and 25%, respectively, on tweets and Amazon reviews datasets. These findings indicate ChatGPT's potential as a valuable tool for annotating sentiment analysis data.","This study evaluates ChatGPT as a data labeling tool for sentiment analysis, demonstrating its superiority over lexicon-based methods with accuracy improvements of 20% and 25% on tweets and Amazon reviews datasets, respectively, highlighting its potential in diverse sentiment analysis applications.","ChatGPT, a powerful AI tool by OpenAI, outperforms lexicon-based methods in sentiment analysis tasks, showing significant accuracy improvements of up to 25%. Its versatility and accuracy make it a valuable tool for data labeling in various sentiment analysis applications, surpassing existing approaches.",Leveraging ChatGPT As Text Annotation Tool For Sentiment AnalysisSentiment analysis is a well-known natural language processing task that involves identifying the emotional tone or polarity of
5,"ChatGPT is nondeterministic, which means identical input can lead to different outputs . this study investigates the consistency of ChatGPT’s zero-shot capabilities for text annotation","ChatGPT's reliability for text annotation and classification is questionable due to its nondeterministic nature. Even minor prompt variations or input repetitions can lead to varying outputs. While pooling outputs may improve reliability, caution is advised. Unsupervised use of ChatGPT for these tasks is not recommended, and thorough validation against human-annotated data is crucial.","This study highlights the inconsistent reliability of ChatGPT in zero-shot text annotation and classification tasks, showing that even minor prompt changes or repeated inputs can yield varied results, and advises thorough validation against human-annotated data.","ChatGPT shows promise for text tasks, but its nondeterministic nature raises reliability concerns. Testing its zero-shot capabilities reveals inconsistencies, with minor changes leading to varied outputs. Caution is advised when using ChatGPT for text annotation, emphasizing the importance of validation against human-annotated data.",Testing the Reliability of ChatGPT for Text Annotation and Classification: A Cautionary RemarkRecent studies have demonstrated promising potential of Chat GPT for various text annotation and classification
6,a language model called GPT-3 has improved learning across many tasks . it costs 50% to 96% less to use pseudo labels from GPT-3 than from humans .,"GPT-3, a large language model, can be used as a cost-effective data labeler for NLP tasks. By leveraging GPT-3's labels, models can achieve similar performance with 50-96% less labeling cost compared to human-labeled data. A framework combining GPT-3 and human labels further enhances performance within limited labeling budgets.","This paper explores using GPT-3 as a cost-effective alternative for data labeling in NLP tasks, achieving significant cost reductions (50%-96%) compared to human labeling and enhancing performance through a novel framework combining GPT-3 and human-generated labels.","Leveraging GPT-3 for data labeling significantly reduces costs by 50% to 96% compared to human labeling. Combining GPT-3 pseudo labels with human labels enhances model performance, offering a cost-effective and scalable data labeling solution applicable across various NLP tasks.",Want To Reduce Labeling Cost? GPT-3 Can HelpData annotation is a time-consuming and labor-intensive process for many NLP tasks. Although there exist various methods to
7,automatic genre identification (AGI) is a text classification task focused on genres . obtaining genre information has been shown to be beneficial for a wide range of disciplines .,"Automatic genre identification (AGI) has become increasingly important for disciplines like linguistics and natural language processing. This survey provides an overview of different approaches to AGI, including genre schema definition, dataset collection, and machine learning strategies. It highlights the most relevant genre schemata and datasets, discusses recent advances in machine learning approaches, and proposes directions for developing a stable multilingual genre classifier.","The research paper surveys Automatic Genre Identification (AGI), highlighting its importance across multiple disciplines and the variability in genre datasets due to differing definitions and annotation methods. It details AGI methodologies, key datasets, and advances in machine learning for AGI, proposing future directions for a stable multilingual classifier.","""Automatic genre identification (AGI) involves classifying text based on genres defined by purpose, function, and form. Researchers have created diverse genre datasets for developing efficient classifiers, emphasizing the importance of understanding dataset differences. The paper reviews AGI approaches, genre schemata, datasets, and machine learning strategies for multilingual classification.""","The title of the research paper is ""Automatic genre identification: a study of machine learning and machine learning approaches to automatic genre identification (AGI).Automatic genres identification (AAI"
8,research paper compares quality of annotations in low-resource language NLP tasks . human annotations outperformed those produced by large language models (LLMs) findings highlight need,"This study compares the quality of annotations in Turkish, Indonesian, and Minangkabau NLP tasks generated by human annotators and LLMs. Human-generated annotations outperformed LLM-generated ones in complex tasks, while LLMs showed competitive quality in sentiment analysis. This highlights the need for judicious selection of annotation sources and continued advancements in LLM capabilities.","This study evaluates annotation quality in Turkish, Indonesian, and Minangkabau NLP tasks, comparing human-generated annotations to those from Large Language Models (LLMs). Findings reveal humans outperform LLMs in complex tasks, highlighting LLMs' contextual limitations.","This study compares human and Large Language Model (LLM)-generated annotations in Turkish, Indonesian, and Minangkabau NLP tasks. While LLM annotations excel in sentiment analysis, human annotations outperform in complex tasks, emphasizing the need for improved LLM capabilities in data annotation for NLP and machine learning.",ChatGPT Label: Comparing the Quality of Human-Generated and LLM- Generated Annotations in Low-Resource Language NLP TasksThis research paper presents a comprehensive comparative
9,large language models like ChatGPT can learn in-context (ICL) from examples . this is the first study that assesses ChatGPT 's effectiveness in an,"Researchers demonstrate the effectiveness of ChatGPT in annotating a dataset for training instructor models in intelligent tutoring systems. ChatGPT's in-context learning capabilities allow it to select appropriate tutoring instructions based on student states, offering a potential solution to the time-consuming and expensive process of human annotation.","This study evaluates ChatGPT's effectiveness in annotating datasets for intelligent tutoring systems (ITSs), presenting a novel, cost-effective method that could replace human experts, alongside contributions including a new dataset annotation methodology and a publicly available annotated dataset.","This study explores ChatGPT's effectiveness in annotating datasets for intelligent tutoring systems. By leveraging prompt engineering, ChatGPT selects tutoring instructions for student states, proving to be a cost-effective alternative to human experts. The study introduces a novel dataset annotation methodology and a dataset of annotated student states.",Using ChatGPT to annotate a dataset: A case study in intelligent tutoring systemsABSTRACT  Â ÂLarge language models like ChatGPN can learn in-
10,"manual annotation can provide better-quality data, especially in building personalized models . ChatGPT-Emo is an artificial corpus created and annotated entirely by ChatGPT","Researchers explored the use of ChatGPT to annotate texts with emotions and generate synthetic texts with annotations. They created three corpora with varying annotation sources: human, ChatGPT, and fully ChatGPT-generated. Fine-tuning Transformer models on these corpora revealed that human annotations yield higher data quality, particularly for personalized models.","This paper explores the feasibility of using ChatGPT for automatic text annotation and generating annotated artificial texts. It compares three text collections—CLARIN-Emo, Stockbrief-GPT, and ChatGPT-Emo—analyzing their quality and the effectiveness of Transformer-based models trained on them.","This paper explores training emotion recognition models using human annotation and ChatGPT. Three text collections were prepared: CLARIN-Emo with manual annotations by linguists, Stockbrief-GPT with ChatGPT annotations, and ChatGPT-Emo with artificial annotations. Manual annotation yields higher-quality data for personalized models.","CLARIN-Emo: Training Emotion Recognition Models using Human Annotation and ChatGPTIn this paper, we investigate whether it is possible to automatically annotate texts with"
11,a dataset of debatepedia arguments and counter-arguments is limited by noise . even most queries in this dataset do not have any relevance to the respective document . we harness the,"CQSumDP enhances the Debatepedia dataset for query-focused abstractive summarization by leveraging ChatGPT to regenerate queries, addressing the dataset's noise and low query relevance. Evaluations show that the annotated version outperforms the original in query relevance and summary quality.","CQSumDP enhances Debatepedia for query-focused summarization by using ChatGPT to regenerate relevant queries, significantly improving summary quality and query relevance, with plans to release this improved dataset publicly.","This paper introduces CQSumDP, a ChatGPT-annotated resource for query-focused abstractive summarization based on Debatepedia. By leveraging ChatGPT's language generation capabilities, the cleaned dataset outperforms the original in query relevance and summary quality, promising a valuable resource for researchers.",CQSumDP: A ChatGPT-Annotated Resource for Query-Focused Abstractive Summarization Based on DebatepediaDebatepedia is a publicly available dataset consisting
12,"paper examines the hierarchical and multilayered taxonomy of Speech Functions . pragmatics, turn-taking, feedback, and topic switching are analyzed .","This paper explores the use of ChatGPT to generate synthetic data for linguistic annotation of casual conversations, focusing on the hierarchical taxonomy of Speech Functions. Comparative analyses involving expert annotators and crowdsourcing workers evaluate the distinctiveness of pragmatic classes. Experiments on manually and synthetically annotated datasets demonstrate the feasibility of using ChatGPT for complex discourse annotation tasks. The findings contribute to prompt engineering techniques for linguistic annotation in large language models.","This paper explores the complex taxonomy of Speech Functions in conversations, analyzing pragmatic classes through expert and crowdsourced annotations, and testing ChatGPT's effectiveness in generating synthetic datasets for advanced discourse annotation, enhancing dialogue system development.","This paper explores Speech Functions taxonomy in open-domain conversations, comparing pragmatic classes using expert annotators and crowdsourcing. It conducts classification experiments on annotated and synthetic datasets created with ChatGPT, highlighting the potential of using AI for discourse data generation and advancing prompt engineering techniques for linguistic annotation.",Linguistic Annotation Generation with ChatGPT: a Synthetic Dataset of Speech Functions for Discourse Annotation of Casual ConversationsThis paper is devoted to examining the hierarchical
13,the large language model (LLM) ChatGPT-4 is compared to manual annotation by both expert classifiers and crowd workers . the paper uses tweet messages from United States politicians during,"In a study comparing ChatGPT-4's text analysis accuracy to human annotators, ChatGPT-4 surpassed experts and crowd workers in classifying political affiliations in Twitter messages with zero-shot learning. The LLM demonstrated higher accuracy, reliability, and equal or lower bias than human annotators, suggesting significant potential for LLM use in social science research involving textual data.","This study evaluates ChatGPT-4's performance in classifying political affiliations from Twitter messages against expert and crowd worker annotations. ChatGPT-4 demonstrated superior accuracy, reliability, and comparable or reduced bias, suggesting significant implications for social science research.","ChatGPT-4 surpasses human experts and crowd workers in classifying political Twitter messages accurately, reliably, and with lower bias. It demonstrates superior performance in annotating tweets based on contextual knowledge and author intentions, indicating its potential to revolutionize interpretive research in social sciences.","ChatGPT-4 Outperforms Experts and Crowd Workers in Annotating Political Twitter Messages with Zero-Shot LearningThis paper assesses the accuracy, reliability, and bias of the"
14,"a systematic literature review assesses the merits and limits of chatGPT . the few-shot learners offer enticing, yet mixed results on text annotation tasks .","Recent studies suggest that zero-shot text annotation models like ChatGPT may offer mixed performance, with varying accuracy levels. While these models have potential, their effectiveness remains uncertain, raising concerns about reproducibility, privacy, copyright, and the dominance of English. Therefore, it's crucial to approach their use with caution and evaluate their merits carefully.","Researchers have debated the potential of zero- or few-shot classifiers like ChatGPT for text annotation, revealing mixed performance results. Systematic reviews indicate these models often underperform compared to those fine-tuned with human annotations, raising concerns about result reproducibility, privacy, copyright, and language biases.","Researchers are exploring the potential of zero- or few-shot classifiers like ChatGPT for text annotation, but findings show mixed results. While promising, these models often underperform compared to human-annotated models. The effectiveness of these models remains uncertain, raising questions about reproducibility and ethical considerations.","ChatGPT for Text Annotation? Mind the Hype!In the past months, researchers have enthusiastically discussed the relevance of zero- or few-shot classifiers like ChatGPT"
