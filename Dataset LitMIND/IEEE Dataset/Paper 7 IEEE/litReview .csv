Related_Work
"In recent years, as the growing field of sequential recommendation evolves, this study introduces a novel Personalized Prompt-based Recommendation (PPR) framework. By integrating prompt-tuning from NLP with recommendation systems, PPR addresses the challenges of cold-start scenarios through a personalized prompt generator and prompt-oriented contrastive learning. Proven effective and efficient across various tasks, PPR shows significant improvements in few-shot and zero-shot scenarios, and is adaptable to different models and tasks, including cross-domain recommendation and user profile prediction.Sun et al. proposed BERT4Rec, enhancing recommendation accuracy with bidirectional self-attention, outperforming unidirectional models. [1] Schick & Sch√ºtze proposed PET to improve text classification and inference using cloze-style prompts. [2]. Gu et al. proposed Pre-trained Prompt Tuning (PPT) that enhances few-shot learning, outperforming full-model fine-tuning. [3] Liu et al. proposed P-Tuning to enhance and stabilize NLU performance using continuous prompt embeddings. [4]. Liu et al. proposed ASReP to address cold-start issues by augmenting sequences with pseudo-prior items [5]. Li et al. explore personalized prompt learning with Transformers, improving explainable recommendations [6]. Zhou et al. proposed S3-Rec, enhancing sequential recommendations using self-supervised learning and mutual information [7]. Radford et al. demonstrated that large language models perform multiple tasks without supervision using vast datasets [8]. Xie et al. developed CL4SRec using contrastive learning and data augmentation to improve sequential recommendations [9]. Cui et al. proposed M6-Rec to unify and enhance recommender system tasks, improving efficiency and reducing impact. [10] Lian et al. proposed GeoSAN, improving location recommendations by 34.9% using self-attention and geography-aware techniques [11]. Zeng et al. highlighted pre-trained models' effectiveness in mitigating data sparsity in recommender systems [12]. Li & Liang emphasized prefix-tuning as an efficient alternative to fine-tuning, optimizing continuous prompts [13]. Wu et al. proposed PFRec, a framework for selective fairness using prompts and adversarial training [14]. Radford et al. introduced training image recognition models using image-caption pairs for scalable pre-training. [15] Shin et al. demonstrated ShopperBERT's superior efficiency and generalizability in e-commerce tasks [16]. Huang et al. proposed integrating large language models with recommender systems for interactive recommendations, outperforming general LLMs [17]. Brown et al. demonstrated GPT-3's impressive few-shot learning abilities across various NLP tasks. [18] Zheng et al. introduced Mecos to address item cold-start in sequential recommendations, improving HR@10 by 99% [19]. He et al. proposed Masked Autoencoders (MAE) for scalable self-supervised learning in vision. [20]. Hou et al. proposed UniSRec for transferable recommendations using item descriptions to enhance recommender systems. [21] Xiao et al. proposed UPRec to enhance recommendation models using user attributes and social graphs [22]. Wu et al. proposed PPR, a personalized prompt framework, enhancing cold-start recommendations across tasks. [23] Hidasi et al. emphasized RNNs outperform traditional methods in session-based recommendations by modeling entire sessions [24]. Chen et al. proposed UAF to enhance cross-domain recommendations for cold-start users. [25]. Yao et al. proposed a self-supervised learning framework to enhance large-scale item recommendations [26]. Wu et al. argue SGL enhances GCNs' recommendation accuracy and robustness. [27] Kang & McAuley proposed SASRec, a self-attention model that captures long-term dependencies efficiently. [28] Vaswani et al. introduced the Transformer, achieving superior translation quality and efficiency using attention mechanisms. [29] Jiang et al. improved language model accuracy from 31.1% to 39.6% using better prompts. [30] Yuan et al. discussed PeterRec's impact on transfer learning in recommendation tasks, maintaining pre-trained parameters. [31] Han et al. proposed GUESR using graph contrastive learning and bucket-cluster sampling to enhance sequential recommendation. [32] Liu et al. emphasized RoBERTa's optimization of hyperparameters and data size, achieving state-of-the-art results. [33] Gao et al. emphasized LM-BFF's 30% improvement in few-shot learning for smaller language models [34]. Liu et al. proposed P-Tuning v2, achieving fine-tuning performance with minimal parameters across NLP tasks. [35] Geng et al. proposed a unified text-to-text paradigm for recommendation tasks, enhancing personalization and transferability [36]."
