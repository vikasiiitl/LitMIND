Related_Work
"In recent years, as the growing field of NLP intersects with supply chain management, this study investigates transformer models like BERT and RoBERTa for Named Entity Recognition in Australia's construction sector. RoBERTa, with the highest F1 score, shows promising applications in identifying supply chain risks, enhancing risk management strategies.Ndukwe et al. examined the impact of COVID-19 on the China-Australia construction supply chain. Lockdowns led to production delays, increased shipping costs, and four-week delays, highlighting vulnerabilities and emphasizing the need for effective future disruption management strategies [1]. Bengio et al. proposed a neural probabilistic language model addressing the curse of dimensionality by learning distributed word representations. This model generalizes to unseen sequences using word similarities, significantly outperforming traditional trigram models in experiments on two text corpora [2]. Mikolov et al. introduced two innovative architectures for learning continuous word embeddings, surpassing existing neural network techniques in word similarity tasks. These models achieve state-of-the-art performance with reduced computational cost, processing 1.6 billion words in under a day [3]. Yang et al. introduced BBIEC, a BERT-based Chinese named entity recognition model for COVID-19 data analysis. Utilizing BERT, BiLSTM, IDCNN, and CRF, the model excels in identifying key entities and tracing transmission routes, outperforming traditional methods [4]. Johnson et al. focus on word embedding techniques, especially word2vec, to handle vast textual data in natural language processing. The paper provides a comprehensive guide on strategies, theoretical foundations, and advancements, enhancing researchers' understanding and application [5]. Qiu et al. developed a workflow using text mining, natural language processing, and gazetteer matching to extract spatiotemporal and semantic information from unstructured Chinese geological documents. This method enhances geological knowledge representation and facilitates research through improved analysis and keyword suggestions [6]. Almeida & Xexéo discuss strategies for creating word embeddings, which are fixed-length, dense representations capturing syntactic and semantic information. These embeddings improve performance in various NLP tasks, demonstrating their importance in natural language processing applications. [7] Zhu & Zhou proposed a novel agricultural named entity recognition model, PBERT-BILSTM-CRF, using pre-trained BERT, BILSTM, and CRF. It addresses traditional method limitations, achieving a 90.24% F1 score, enhancing precision, recall, and processing speed in agricultural text tasks [8]. Tang et al. propose a multi-task BERT-BiLSTM-AM-CRF model for Chinese Named Entity Recognition, enhancing performance by leveraging dynamic word vectors and contextual semantics. The model achieves significant F1 score improvements on MASR and People’s Daily datasets, demonstrating its effectiveness [9]. Mikolov et al. introduced extensions to the Skip-gram model, improving word vector quality and training speed through subsampling and negative sampling. They addressed single-word limitations by learning vectors for phrases, capturing idiomatic expressions like ""Air Canada"" [10]. Shishehgarkhaneh et al. analyzed transformer models like BERT and RoBERTa for Named Entity Recognition in Australian construction supply chain risk management. RoBERTa achieved the highest accuracy with an F1 score of 0.8580, highlighting NLP's potential in this field [11]. Huang et al. utilized the BERT pre-trained language model for Geological News Named Entity Recognition (GNNER), improving context comprehension and extraction accuracy. The model achieved an average F1 score of 0.839, effectively identifying six geological entities in Chinese news. [12]. Lv et al. propose BERT-BiGRU-CRF, a deep learning model for geological named entity recognition, addressing linguistic irregularities in geoscience texts. It outperforms five baseline models on four datasets, enhancing complex entity extraction from geological reports [13]. The study by Chih-Ming Tsai introduces the NER-SA model, integrating NLP and named entity recognition to enhance fake news detection. It outperforms existing methods, improving accuracy by up to 19.36% in-domain and 28.51% cross-domain. [14] Jarrar et al. introduced Wojood, an Arabic corpus for nested Named Entity Recognition (NER) with 550K tokens and 21 entity types, 22.5% of which are nested. It achieved high inter-annotator agreement and supports AraBERT-based NER models. [15] Vaswani et al. introduced the Transformer, a novel architecture using only attention mechanisms, surpassing traditional recurrent and convolutional models. It achieves superior BLEU scores of 28.4 and 41.0 for English-to-German and English-to-French translations, respectively, with improved parallelization and reduced training time [16]. Berragan et al. developed five custom NER models for extracting place names, achieving an F1 score of 0.939, outperforming pre-built models. The models effectively identify unknown place names in online geographic data, validated using annotated Wikipedia articles [17]. Yang et al. developed an NER model using RoBERTa, biLSTM, and CRF to identify Traditional Chinese Medicine and disease names, achieving high precision, recall, and F1-score of 0.96, enhancing medication reminders in dialogue systems [18]."
