Related_Work
"Numerous studies have addressed this topic with varying approaches. This article presents an automated NLP-based framework to enhance traffic reporting systems by mining social media for real-time alerts, utilizing advanced models like Bidirectional Encoder Representations from Transformers for data classification and extraction.Dumford & Scheirer investigated a white-box backdoor attack on convolutional neural networks (CNNs) targeting facial recognition systems. By altering CNN weights, the attack manipulates recognition processes, increasing false acceptances without impacting legitimate accuracy, underscoring critical security gaps [1]. Gu et al. discussed how deep neural networks, vulnerable to backdoor attacks when outsourced for training, can be manipulated to misclassify specific inputs. This highlights the need for enhanced verification and inspection methods to secure networks against such threats [2]. Liu et al. discuss Refool, a stealthy backdoor attack using reflections to compromise DNNs during training, achieving high success and evading defenses across multiple datasets [3]. Chen et al. investigated backdoor attacks on deep learning systems, demonstrating that injecting minimal poisoned samples into training sets can achieve over 90% success. These findings expose significant vulnerabilities and underscore the urgent need for enhanced security measures [4]. Gongora Svartzman & Ramirez Marquez investigate public transportation disruptions during a 2018 snowstorm on New York's Metro North Railroad, comparing official data and passenger reports using Natural Language Processing and visual analytics [5]. Qi et al. proposed ONION, a novel defense against textual backdoor attacks in deep neural networks, utilizing outlier word detection to safeguard models like BiLSTM and BERT from various attacks. ONION is effective across all textual backdoor attack scenarios and is available online [6]. Li et al. investigated how NLP systems are compromised by backdoor attacks using covert triggers like homograph replacement and subtle textual differences. These attacks manipulate tasks such as toxic comment detection and machine translation, achieving high success rates while evading human detection [7]. Turner et al. discuss how adversaries inject malicious inputs into training data to control model behavior through backdoor triggers. They present a method using adversarial perturbations and generative models to create stealthy, label-consistent backdoor attacks, making detection difficult [8]. Lucic et al. discuss how crowdsourcing in Intelligent Transportation Systems (ITS) and Vehicular Social Networks (VSN) utilizes mobile, spatial, and passive sensing to improve infrastructure monitoring, navigation, and congestion management [9]. Dai et al. investigate backdoor attacks on LSTM-based text classification, where adversaries inject backdoors via data poisoning, causing misclassification with specific triggers, achieving a 96% success rate while maintaining overall model performance [10]. Sun et al. investigated the vulnerability of NLG systems to backdoor attacks, proposing defense strategies for tasks like machine translation and dialog generation. They emphasized the importance of further research to secure NLG systems against generating offensive content [11]. Li et al. introduced DeepPayload, a backdoor attack on deep learning models in mobile apps, achieving a 93.5% success rate with minimal performance impact, highlighting the need for enhanced model protection [12]. Chen & Dai discussed backdoor attacks in LSTM-based text classification systems, introducing a defense method called Backdoor Keyword Identification (BKI) which identifies and removes poisoned training samples, effectively enhancing model security across multiple datasets [13]. Li et al. investigated the vulnerability of NLP models to backdoor attacks, proposing two novel methods: a multistyle transfer-based attack and a paraphrase-based attack, both demonstrating effective invisibility and semantic consistency [14]. Qi et al. investigated adversarial and backdoor attacks in NLP through text style transfer, revealing that altering text style while preserving content significantly compromises NLP models, with success rates over 90% [15]. Cheng et al. discuss a novel deep feature space Trojan attack on neural networks, highlighting its effectiveness, stealthiness, controllability, and robustness. This attack evades advanced defenses across multiple image classifiers and datasets, using deep feature-reliant triggers. Extensive experiments on 9 image classifiers, including ImageNet, demonstrate its capability to bypass state-of-the-art defenses [16]. Rakin et al. introduced the Targeted Bit Trojan (TBT), a novel method that inserts a neural Trojan into Deep Neural Networks using bit-flip attacks, efficiently manipulating DNN weights to misclassify inputs with high success rates and minimal bit-flips [17]. Qi et al. investigated backdoor attacks in NLP, introducing a novel textual backdoor attack using syntactic structures as triggers, achieving nearly 100% success with enhanced invisibility and resistance to defenses [18]. MÃ©ndez et al. propose a novel methodology using Twitter data and text mining techniques like sentiment analysis and topic modeling to assess public transport user satisfaction in Santiago, providing broader coverage and timely insights compared to traditional surveys [19]. Kurita et al. discuss ""weight poisoning"" in pre-trained NLP models, where vulnerabilities embedded in model weights create manipulative backdoors. They demonstrate the attack's broad applicability and propose defensive strategies against such threats across various NLP tasks [20]. Liu et al. investigated the vulnerability of neural networks to Trojan attacks, where malicious triggers manipulate model behavior during training, emphasizing the importance of defense mechanisms for network security and reliability [21]. Shao et al. introduced BDDR, a defense against textual backdoor attacks in DNNs, reducing attack success rates by over 90% for word-level and 60% for sentence-level through detection and text reconstruction using a pre-trained BERT model [22]. Hochreiter & Schmidhuber developed Long Short-Term Memory (LSTM) to address challenges in storing information over long intervals identified in 1991. LSTM, a gradient-based method, maintains constant error flow, significantly enhancing performance in complex tasks with long time lags [23]. Chen et al. investigated improving public service satisfaction in Shanghai's suburb parks through monitoring online reviews and applying sentiment analysis and machine learning. This approach aids in decision-making and enhances service quality by visualizing public satisfaction [24]. Pan et al. investigated the Hidden Trigger Backdoor Attack on NLP Models through Linguistic Style Manipulation [25]."
