Related_Work
"Recent research highlights significant advances in multi-intent Spoken Language Understanding (SLU), introducing the Co-guiding Net model that enhances mutual task guidance and achieves a 21.3% accuracy improvement.Wang et al. propose a Relational Graph Attention Network (R-GAT) for aspect-based sentiment analysis, utilizing a modified dependency tree to enhance connections between aspects and opinion words, thereby improving sentiment prediction accuracy, as demonstrated on benchmark datasets [1]. Gangadharaiah & Narayanaswamy developed an attention-based neural network that combines intent detection and slot labeling, achieving state-of-the-art results and significant improvements on both single and multi-intent datasets [2]. Qin et al. proposed a Co-Interactive Transformer for slot filling and intent detection in spoken language understanding, using a novel module to model bidirectional task relationships, significantly improving performance on public datasets [3]. Wu et al. propose SlotRefine, a fast non-autoregressive model for joint intent detection and slot filling in spoken language understanding systems. It features a two-pass mechanism that significantly enhances speed and accuracy, further improved by pretraining [4]. E et al. propose a novel bi-directional interrelated model for joint intent detection and slot filling in spoken language understanding systems. Utilizing an SF-ID network with an iterative mechanism, the model significantly enhances accuracy on ATIS and Snips datasets [5]. Zhang & Wang propose a joint model for intent determination and slot filling in spoken language understanding, utilizing GRUs and max-pooling. The model demonstrates superior performance over existing methods on two datasets [6]. Qin et al. developed the GL-CL EF, a novel framework for zero-shot cross-lingual spoken language understanding, using contrastive learning and bilingual dictionaries for explicit alignment of sentence representations, significantly outperforming existing models on MultiATIS++ [7]. Hakkani-TÃ¼r et al. propose a holistic multi-domain, multi-task RNN-LSTM model for semantic frame parsing, effectively handling slot filling, intent, and domain classification with superior performance demonstrated on Cortana data, outperforming traditional methods [8]. Ni et al. review state-of-the-art research in deep learning-based dialogue systems, covering principles, characteristics, applications, evaluation methods, and datasets. They analyze both task-oriented and open-domain models, providing a comprehensive resource for researchers [9]. Xing & Tsang introduced Co-Guiding Net, a novel model for multi-intent spoken language understanding using a two-stage framework and heterogeneous graph attention networks, achieving significant accuracy improvements, including 21.3% on MixATIS and 33.5% on zero-shot cross-lingual tasks [10]. Xing & Tsang introduced ReLaNet, a model using a Heterogeneous Label Graph to enhance semantic-label interactions for joint multiple intent detection and slot filling, significantly outperforming existing models on the MixATIS dataset by over 20% in accuracy [11]. Goo et al. proposed a slot-gated model that enhances joint slot filling and intent prediction by linking intent and slot attention vectors, achieving a 4.2% and 1.9% improvement on ATIS and Snips datasets respectively [12]. Xing & Tsang proposed DARER, a dual-task network utilizing speaker-aware and relational temporal graphs for joint dialog sentiment classification and act recognition, significantly enhancing performance and efficiency [13]. Zhou et al. propose KNN-Contrastive Learning to enhance Out-of-Domain (OOD) intent classification in dialogue systems by leveraging K-Nearest Neighbors to learn discriminative features, significantly improving OOD detection and In-Domain accuracy without assuming feature distributions, validated by extensive experiments [14]. Xing & Tsang proposed a novel multi-task learning (MTL) framework, the Co-evolving Graph Reasoning Network (CGR-Net), for Emotion-Cause Pair Extraction (ECPE). This framework enhances task interaction and captures causal dependencies through a relational graph, achieving state-of-the-art performance [15]. Xing & Tsang introduced a novel knowledge-aware model for aspect-level sentiment classification (ASC), integrating dual syntactic information and an attention mechanism, significantly outperforming existing models on benchmark datasets [16]. Xing & Tsang's Neural Subgraph Explorer model enhances syntax-based sentiment classification by pruning irrelevant nodes and introducing target-related connections, achieving superior performance and setting new benchmarks through a novel multi-layer graph convolution approach [17]. Qin et al. introduced GL-GIN, a non-autoregressive model for joint multiple intent detection and slot filling, which outperforms existing models by leveraging a novel graph interaction network to enhance speed and accuracy, addressing slow inference and information leakage [18]. MuGNN, a multi-channel graph neural network, addresses entity alignment in knowledge graphs by leveraging diverse relation weighting, self- and cross-KG attention, and rule knowledge transfer. It significantly improves alignment accuracy, outperforming existing methods by 5% on average [19]. Shi et al. proposed TransferNet, a novel multi-hop QA framework that unifies label and text relations in a transparent, differentiable process. It excels in accuracy and interpretability, outperforming existing models on MetaQA's multi-hop questions through effective entity relation handling and score transferring [20]. Schlichtkrull et al. introduced Relational Graph Convolutional Networks (R-GCNs) to enhance knowledge base completion, effectively handling multi-relational data and significantly boosting performance in tasks like link prediction and entity classification by incorporating relational graph structures [21]. Kim et al. developed a two-stage system to detect multiple intents in sentences using single-intent labeled data. The system categorizes sentences, uses conjunctions for hypothesis generation, and employs sequence labeling, significantly reducing errors in both written and spoken language [22]. Qin et al. proposed the Adaptive Graph-Interactive Framework (AGIF) for joint multiple intent detection and slot filling, utilizing an intent-slot graph interaction layer. This framework adaptively integrates fine-grained intent information, enhancing token-level slot prediction and achieving state-of-the-art results on various datasets [23]. Li et al. proposed a self-attentive model with a gate mechanism for Spoken Language Understanding, leveraging intent semantics for enhanced slot filling. This model achieves state-of-the-art results on the ATIS dataset, significantly outperforming existing methods in intent detection and slot filling [24]. Xing & Tsang developed DigNet, a neural network that enhances aspect-level sentiment classification by integrating local syntactic and global relational information through a local-global interactive graph and LGI layers, achieving superior results on benchmarks [25]. Wang et al. proposed a new method, Hierarchy-guided Contrastive Learning (HGCLR), for hierarchical text classification that embeds label hierarchies directly into text encoders. This approach uses hierarchy to guide the creation of positive samples, enhancing text representation without separate hierarchy encoding, and shows effectiveness on three benchmark datasets [26]. Qin et al. propose a Stack-Propagation framework for spoken language understanding, enhancing slot filling by integrating token-level intent detection and BERT, achieving state-of-the-art results on public datasets [27]. Xing & Tsang proposed a dual-task dialogue understanding framework using speaker-aware temporal graph reasoning, featuring models DARER and DARER2. These models significantly outperform existing ones in dialog sentiment classification, achieving improvements of 28% and 34% on the Mastodon dataset [28]. Zhang et al. propose a capsule-based neural network for joint slot filling and intent detection in natural language understanding, utilizing dynamic routing and re-routing schemas to capture hierarchical relationships, outperforming existing models [29]. Liu et al. introduced CM-Net, a Collaborative Memory Network that enhances Spoken Language Understanding by leveraging slot-intent co-occurrence. Using CM-blocks for feature enrichment, CM-Net achieves state-of-the-art results on ATIS, SNIPS, and CAIS datasets [30]. Zhang et al. propose an aspect-specific Graph Convolutional Network (GCN) that utilizes dependency trees to enhance aspect-based sentiment classification by capturing syntactical information and long-range word dependencies. Experiments show its superiority over traditional attention and CNN-based models [31]. Zhang et al. propose PairSupCon, a pairwise supervised contrast learning approach that enhances sentence representation by integrating semantic entailment and contradiction with categorical concept encoding, significantly outperforming existing methods in clustering and semantic textual similarity tasks [32]."
