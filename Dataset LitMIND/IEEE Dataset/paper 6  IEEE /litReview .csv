Related_Work
"In recent years, as the growing field of legal AI advances, the HANOI-Legal system proposes a parallel learning (PL) framework to address the challenges of domain-specific tasks. Utilizing a unified prompting method, UniPrompt, and leveraging open datasets, HANOI-Legal enhances pretrained language models for effective use in low-resource legal tasks, showing superior performance in experimental validations.Song et al. proposed HANOI-Legal, a novel approach using a parallel learning framework and UniPrompt to enhance PLMs for legal tasks. It outperforms existing methods in low-resource scenarios by leveraging diverse datasets and fine-tuning with DAO-II [1]. Brown et al. introduced GPT-3, a 175 billion parameter model, demonstrating impressive few-shot learning without fine-tuning. Despite strong results in various tasks, GPT-3 struggles with some datasets and raises ethical concerns due to generating realistic text, impacting news authenticity [2]. Sanh et al. explored explicit multitask learning to enhance zero-shot task generalization in language models. By converting datasets into prompted formats and fine-tuning a model, they achieved strong performance exceeding larger models, indicating prompted training's effectiveness in zero-shot generalization [3]. Liu et al. proposed P-Tuning, a method using trainable continuous prompt embeddings with discrete prompts, to stabilize training and improve performance in NLU tasks like LAMA and SuperGLUE. It benefits both frozen and tuned models in various settings [4]. Wu et al. developed a method for abstractive book summarization using human feedback and recursive task decomposition. This approach achieves state-of-the-art results on the BookSum dataset and excels in question answering on the NarrativeQA benchmark [5]. Li et al. propose a ""parallel learning"" framework to address inefficiencies in machine learning for complex systems. By integrating descriptive, predictive, and prescriptive learning, this method enhances exploration and provides a unified theory for real-world problems. [6] Ziegler et al. explored reward learning for language tasks using human feedback to fine-tune models for text continuation and summarization. Despite promising results with minimal evaluations, concerns about exploiting labeling heuristics remain, emphasizing careful evaluation in RL settings [7]. Wang et al. explore artificial intelligence origins, introducing hybrid and parallel intelligence concepts. They highlight a paradigm shift in cyber-social-physical systems, guided by Merton's Laws, proposing ACP-based parallel intelligence to address modeling gaps and enhance understanding of CPSS [8]. Li et al. highlight that AI approaches often sacrifice interpretability for accuracy, leading to trust issues and risks. They propose ""scenarios engineering"" with six dimensions, including intelligence, calibration, and validation, to enhance AI robustness and trustworthiness [9]. Liu et al. discuss prompt-based learning, a new NLP paradigm leveraging pre-trained language models to perform few-shot and zero-shot learning by crafting input prompts. This survey organizes research on language model choices, prompt design, and tuning strategies [10]. Li & Liang introduced prefix-tuning, a lightweight alternative to fine-tuning for natural language generation. It optimizes task-specific vectors, keeping model parameters frozen, achieving comparable performance with minimal parameter change and excelling in low-data scenarios [11]. Khashabi et al. emphasize UNIFIED QA, a pre-trained model excelling across 20 QA datasets and four formats, matching specialized models. It shows impressive generalization on 12 unseen datasets and achieves state-of-the-art results when fine-tuned, challenging format-specific approaches [12]. Li et al. proposed a novel framework using computer graphics to generate synthetic video datasets, ""Synthetic-HS (CG)"", for highway surveillance. The framework enhances foreground detection accuracy with realistic videos, automatic labeling, and improved visual perception using an image translation model [13]. Devlin et al. introduced BERT, a novel language model utilizing bidirectional transformers for deep contextual embeddings from unlabeled text. BERT achieves state-of-the-art performance across 11 NLP tasks with minimal modifications, significantly improving benchmarks like GLUE, MultiNLI, and SQuAD [14]. Min et al. explored in-context learning in large language models, revealing that accurate demonstrations are not essential for task performance. Key factors include label space, input distribution, and sequence format, raising questions about LLM capabilities and inference-based learning [15]. Xu et al. proposed ZeroPrompt, a multitask pretraining approach scaling to 1,000 tasks, enhancing zero-shot learning efficiency by 30 times in FLOPs. This method proves task scaling is more effective than model scaling, improving performance across datasets [16]. Li et al. propose a novel Scenarios Engineering framework for foundation models in the metaverse, addressing vulnerabilities like data poisoning with a six-layer architecture. The approach enhances trustworthiness and accessibility, illustrated through automotive industry use cases. [17] Wei et al. discuss large language models exhibiting emergent abilities, unpredictable and absent in smaller models. As models scale, new capabilities defy performance predictions based on smaller models, suggesting potential for further unforeseen functionalities with continued scaling [18]. Wei et al. show that chain-of-thought prompting enhances large language models' reasoning performance on arithmetic, commonsense, and symbolic tasks. Their method enables PaLM 540B to outperform fine-tuned GPT-3, highlighting its potential in complex reasoning tasks [19]. Han et al. introduced Prompt Tuning with Rules (PTR) for text classification, which automates sub-prompt generation using prior knowledge encoded as rules. PTR improves effectiveness and efficiency, outperforming traditional methods in relation classification, entity typing, and intent classification tasks [20]. Qin & Eisner explore using gradient descent to optimize ""soft prompts"" for language models, improving performance in AI tasks. This method reveals untapped factual knowledge and shows random initialization's effectiveness, highlighting cost-effectiveness over previous methods [21]. Qiu et al. provide a comprehensive review of pre-trained models (PTMs) for natural language processing (NLP), exploring their transformative impact, categorization, adaptation for downstream tasks, and future research directions. This survey is a practical guide for researchers and practitioners [22]. Zeng & Wang discuss CPSS's role in enhancing societal security and production efficiency by integrating human and social aspects with technology. This multidisciplinary approach aligns with Karl Popper's theory and promises intelligent enterprises and pervasive intelligent spaces. [23]. Zhong et al. propose ""meta-tuning,"" fine-tuning large language models on 43 datasets for enhanced zero-shot learning. This method outperforms previous approaches, especially with larger models, suggesting that unified datasets improve classification performance and reveal LLMs' true potential [24]. Touvron et al. introduced LLaMA, a collection of open-source language models ranging from 7B to 65B parameters, outperforming GPT-3 on several benchmarks. LLaMA-65B rivals top models, promoting transparency and accessibility in language model research [25]. Dong et al. investigate in-context learning (ICL) as a transformative approach in NLP using large language models (LLMs). The paper defines ICL, explores techniques like training strategies, and discusses applications and challenges, encouraging future research to enhance ICL's capabilities [26]. Miao et al. explore the virtual-to-real paradigm in machine learning, emphasizing parallel learning across domains like computer vision and robotics. They propose a framework and taxonomy, addressing challenges and suggesting future research directions in parallel learning principles. [27] Chung et al. explored instruction finetuning, highlighting task and model size scaling and chain-of-thought data, significantly improving language model performance. Flan-PaLM 540B trained on 1.8K tasks outperforms PaLM 540B by 9.4%, achieving state-of-the-art results [28]."
